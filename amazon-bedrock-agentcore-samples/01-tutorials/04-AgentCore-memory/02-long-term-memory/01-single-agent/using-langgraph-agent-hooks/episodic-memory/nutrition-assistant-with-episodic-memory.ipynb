{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangGraph with AgentCore Memory using Episodic Strategy\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook demonstrates how to integrate Amazon Bedrock AgentCore Memory with **episodic memory strategy** in a conversational AI agent using LangGraph framework. We'll focus on the episodic strategy that captures complete conversation sessions, enabling the agent to recall specific meal planning episodes and track how dietary patterns evolve over time.\n",
    "\n",
    "## Tutorial Details\n",
    "\n",
    "| Information         | Details                                                                          |\n",
    "|:--------------------|:---------------------------------------------------------------------------------|\n",
    "| Tutorial type       | Long-term Conversational                                                        |\n",
    "| Agent usecase       | Nutrition Assistant with Episodic Memory Strategy                               |\n",
    "| Agentic Framework   | LangGraph                                                                        |\n",
    "| LLM model           | Anthropic Claude Sonnet 3.7                                                     |\n",
    "| Tutorial components | AgentCore Memory, Episodic Strategy, LangGraph Hooks, Session-based Episodes  |\n",
    "| Example complexity  | Intermediate                                                                     |\n",
    "\n",
    "You'll learn to:\n",
    "- Create AgentCore Memory with episodic memory strategy\n",
    "- Implement pre/post model hooks for automatic memory storage\n",
    "- Build a nutrition assistant that remembers meal planning sessions\n",
    "- Retrieve and reflect on past conversations\n",
    "- Track dietary patterns over time\n",
    "\n",
    "### Scenario Context\n",
    "\n",
    "In this example, we'll create a **Nutrition Assistant** that remembers complete meal planning sessions using episodic memory strategy. The agent will capture full conversation episodes including recipe discussions, ingredient substitutions, and meal feedback. This enables temporal queries like \"What did I plan last week?\" and pattern analysis of dietary habits.\n",
    "\n",
    "## Architecture\n",
    "\n",
    "<div style=\"text-align:left\">\n",
    "    <img src=\"architecture_episodic.png\" width=\"65%\" />\n",
    "</div>\n",
    "\n",
    "### Why Episodic Memory Strategy for Nutrition?\n",
    "\n",
    "- **Session-based**: Each meal planning conversation is an episode\n",
    "- **Temporal context**: Meals are tied to specific times/occasions\n",
    "- **Pattern learning**: Track how preferences evolve\n",
    "- **Rich recall**: Remember full context of past recommendations\n",
    "\n",
    "### How Episodic Memory Strategy Works\n",
    "\n",
    "The episodic strategy is designed to capture interactions as structured episodes and reflect across these episodes to generate meaningful insights. This strategy records not only what happened, but also the intent, thoughts, and outcome for each episode.\n",
    "\n",
    "#### Three Steps in Episodic Strategy:\n",
    "\n",
    "1. **Extraction** – Identifies useful insights from short-term memory to place into long-term memory as memory records\n",
    "2. **Consolidation** – Determines whether to write useful information to a new record or an existing record\n",
    "3. **Reflection** – Insights are generated across episodes from agent interactions\n",
    "\n",
    "#### Strategy Output:\n",
    "\n",
    "**Episodes** (XML-formatted):\n",
    "- Broken down into: situation, intent, assessment, justification, and episode-level reflection\n",
    "- Analyzed turn-by-turn as the interaction proceeds\n",
    "- Helps understand order of operations and tool use\n",
    "\n",
    "**Reflections** (generated in background):\n",
    "- Consolidate across multiple episodes\n",
    "- Extract broader insights identifying:\n",
    "  - Successful strategies and patterns\n",
    "  - Potential improvements\n",
    "  - Common failure modes\n",
    "  - Lessons learned spanning multiple interactions\n",
    "\n",
    "#### For Nutrition Assistant:\n",
    "\n",
    "- **Episodes**: Each meal planning session (recipes discussed, ingredients, decisions)\n",
    "- **Reflections**: Dietary patterns, favorite cuisines, cooking skill progression\n",
    "- **Turn-by-turn**: Recipe exploration → ingredient questions → substitutions → final choice\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Python 3.10+\n",
    "- AWS account with appropriate permissions\n",
    "- AWS IAM role with appropriate permissions for AgentCore Memory\n",
    "- Access to Amazon Bedrock models\n",
    "\n",
    "Let's get started by setting up our environment!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary libraries from https://github.com/langchain-ai/langchain-aws\n",
    "%pip install -qr requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "\n",
    "# Import LangGraph and LangChain components\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langgraph.store.base import BaseStore\n",
    "import uuid\n",
    "\n",
    "\n",
    "region = os.getenv(\"AWS_REGION\", \"us-east-1\")\n",
    "logging.getLogger(\"nutrition-agent\").setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the AgentCoreMemoryStore that we will use as a store\n",
    "from langgraph_checkpoint_aws import AgentCoreMemoryStore\n",
    "\n",
    "# For this example, we will just use an InMemorySaver to save context.\n",
    "# In production, we highly recommend the AgentCoreMemorySaver as a checkpointer which works seamlessly alongside the memory store\n",
    "# from langgraph_checkpoint_aws import AgentCoreMemorySaver\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from bedrock_agentcore.memory import MemoryClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "\n",
    "# Create IAM role for memory execution\n",
    "iam_client = boto3.client(\"iam\")\n",
    "sts_client = boto3.client(\"sts\")\n",
    "account_id = sts_client.get_caller_identity()[\"Account\"]\n",
    "\n",
    "ROLE_NAME = \"AgentCoreMemoryExecutionRole\"\n",
    "\n",
    "# Trust policy for AgentCore Memory (gamma endpoints)\n",
    "trust_policy = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Principal\": {\n",
    "                \"Service\": [\n",
    "                    \"preprod.genesis-service.aws.internal\",\n",
    "                    \"bedrock-agentcore.amazonaws.com\",\n",
    "                    \"developer.genesis-service.aws.internal\",\n",
    "                ]\n",
    "            },\n",
    "            \"Action\": \"sts:AssumeRole\",\n",
    "        }\n",
    "    ],\n",
    "}\n",
    "\n",
    "# Permissions for Bedrock model invocation\n",
    "permissions_policy = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\"bedrock:InvokeModel\", \"bedrock:InvokeModelWithResponseStream\"],\n",
    "            \"Resource\": [\n",
    "                \"arn:aws:bedrock:*::foundation-model/*\",\n",
    "                \"arn:aws:bedrock:*:*:inference-profile/*\",\n",
    "            ],\n",
    "        }\n",
    "    ],\n",
    "}\n",
    "\n",
    "try:\n",
    "    # Try to get existing role\n",
    "    role = iam_client.get_role(RoleName=ROLE_NAME)\n",
    "    MEMORY_EXECUTION_ROLE_ARN = role[\"Role\"][\"Arn\"]\n",
    "    print(f\"✅ Using existing role: {MEMORY_EXECUTION_ROLE_ARN}\")\n",
    "except iam_client.exceptions.NoSuchEntityException:\n",
    "    # Create role\n",
    "    print(f\"Creating IAM role: {ROLE_NAME}\")\n",
    "    role = iam_client.create_role(\n",
    "        RoleName=ROLE_NAME,\n",
    "        AssumeRolePolicyDocument=json.dumps(trust_policy),\n",
    "        Description=\"Execution role for AgentCore Memory with custom strategies\",\n",
    "    )\n",
    "    MEMORY_EXECUTION_ROLE_ARN = role[\"Role\"][\"Arn\"]\n",
    "\n",
    "    # Attach inline policy\n",
    "    iam_client.put_role_policy(\n",
    "        RoleName=ROLE_NAME,\n",
    "        PolicyName=\"BedrockModelAccess\",\n",
    "        PolicyDocument=json.dumps(permissions_policy),\n",
    "    )\n",
    "    print(f\"✅ Created role: {MEMORY_EXECUTION_ROLE_ARN}\")\n",
    "    print(\"⏳ Waiting 10 seconds for IAM propagation...\")\n",
    "    import time\n",
    "\n",
    "    time.sleep(10)\n",
    "\n",
    "print(f\"\\nRole ARN: {MEMORY_EXECUTION_ROLE_ARN}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_name = \"NutritionAssistantEpisodic\"\n",
    "client = MemoryClient(region_name=region)\n",
    "MODEL_ID = \"us.anthropic.claude-3-7-sonnet-20250219-v1:0\"\n",
    "\n",
    "override_strategy = {\n",
    "    \"customMemoryStrategy\": {\n",
    "        \"name\": \"NutritionEpisodicExtractor\",\n",
    "        \"description\": \"Nutrition assistant with episodic memory for meal planning insights\",\n",
    "        \"namespaces\": [\"nutrition/{actorId}/{sessionId}\"],\n",
    "        \"configuration\": {\n",
    "            \"episodicOverride\": {\n",
    "                \"extraction\": {\n",
    "                    \"modelId\": MODEL_ID,\n",
    "                    \"appendToPrompt\": \"Extract meal planning conversations including recipes discussed, ingredients mentioned, dietary considerations, and user feedback.\",\n",
    "                },\n",
    "                \"consolidation\": {\n",
    "                    \"modelId\": MODEL_ID,\n",
    "                    \"appendToPrompt\": \"Consolidate meal planning sessions into episodes, capturing the flow of recipe exploration and decision-making.\",\n",
    "                },\n",
    "                \"reflection\": {\n",
    "                    \"modelId\": MODEL_ID,\n",
    "                    \"appendToPrompt\": \"Generate insights about dietary patterns, favorite recipes, and how meal preferences evolve over time.\",\n",
    "                    \"namespaces\": [\"nutrition/{actorId}\"],\n",
    "                },\n",
    "            }\n",
    "        },\n",
    "    }\n",
    "}\n",
    "\n",
    "memory = client.create_or_get_memory(\n",
    "    name=memory_name,\n",
    "    description=\"Nutrition assistant with episodic memory for meal planning sessions\",\n",
    "    memory_execution_role_arn=MEMORY_EXECUTION_ROLE_ARN,\n",
    "    strategies=[override_strategy],\n",
    ")\n",
    "memory_id = memory[\"id\"]\n",
    "\n",
    "print(f\"✅ Created episodic memory: {memory_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory Configuration Overview\n",
    "\n",
    "Our AgentCore Episodic Memory setup includes:\n",
    "\n",
    "- **Extraction**: Captures meal planning conversations with recipes, ingredients, and feedback\n",
    "- **Consolidation**: Groups conversations into meal planning episodes\n",
    "- **Reflection**: Generates insights about dietary patterns and preferences over time\n",
    "- **Namespaces**: Organizes episodes by user (`nutrition/{actorId}`)\n",
    "\n",
    "Each conversation session becomes an episode that can be recalled and analyzed.\n",
    "\n",
    "## Step 3: Initialize Memory Store and LLM\n",
    "\n",
    "Now we'll initialize the AgentCore Memory Store and our language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the store to enable long term memory saving and retrieval\n",
    "store = AgentCoreMemoryStore(memory_id=memory_id, region_name=region)\n",
    "\n",
    "# Initialize Bedrock LLM\n",
    "llm = init_chat_model(MODEL_ID, model_provider=\"bedrock_converse\", region_name=region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Implement Memory Hooks\n",
    "\n",
    "We'll create pre and post model hooks to automatically handle memory storage:\n",
    "\n",
    "- **Pre-model hook**: Saves the user message before LLM invocation\n",
    "- **Post-model hook**: Saves the assistant response after LLM invocation\n",
    "\n",
    "### How Memory Processing Works\n",
    "\n",
    "1. Messages are saved to AgentCore Memory with actor_id and session_id\n",
    "2. The episodic strategy processes conversations to create structured episodes\n",
    "3. Episodes are stored in the `nutrition/{actorId}/{sessionId}` namespace with turn-by-turn analysis\n",
    "4. Reflections are generated across episodes and stored in the `nutrition/{actorId}` namespace\n",
    "5. Each episode captures situation, intent, assessment, and conversation flow\n",
    "\n",
    "**Note**: LangChain message types are converted under the hood by the store to AgentCore Memory message types so that they can be properly processed into episodes and reflections.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_model_hook(state, config: RunnableConfig, *, store: BaseStore):\n",
    "    \"\"\"Hook that runs pre-LLM invocation to save the latest human message\"\"\"\n",
    "    actor_id = config[\"configurable\"][\"actor_id\"]\n",
    "    thread_id = config[\"configurable\"][\"thread_id\"]\n",
    "    # Saving the message to the actor and session combination that we get at runtime\n",
    "    namespace = (actor_id, thread_id)\n",
    "\n",
    "    messages = state.get(\"messages\", [])\n",
    "    # Save the last human message we see before LLM invocation\n",
    "    for msg in reversed(messages):\n",
    "        if isinstance(msg, HumanMessage):\n",
    "            store.put(namespace, str(uuid.uuid4()), {\"message\": msg})\n",
    "            break\n",
    "\n",
    "    # For episodic strategy, we just save messages - no retrieval needed\n",
    "    # Episodes and reflections are generated automatically in the background\n",
    "    return {\"messages\": messages}\n",
    "\n",
    "\n",
    "def post_model_hook(state, config: RunnableConfig, *, store: BaseStore):\n",
    "    \"\"\"Hook that runs post-LLM invocation to save the assistant response\"\"\"\n",
    "    actor_id = config[\"configurable\"][\"actor_id\"]\n",
    "    thread_id = config[\"configurable\"][\"thread_id\"]\n",
    "\n",
    "    # Saving the message to the actor and session combination that we get at runtime\n",
    "    namespace = (actor_id, thread_id)\n",
    "\n",
    "    messages = state.get(\"messages\", [])\n",
    "    # Save the LLM's response to AgentCore Memory\n",
    "    for msg in reversed(messages):\n",
    "        if isinstance(msg, AIMessage):\n",
    "            store.put(namespace, str(uuid.uuid4()), {\"message\": msg})\n",
    "            break\n",
    "\n",
    "    return {\"messages\": messages}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Create the LangGraph Agent\n",
    "\n",
    "Now we'll create our nutrition assistant agent using LangGraph's `create_react_agent` with our memory hooks integrated. The tool node will contain just our long term memory retrieval tool and the pre and post model hooks are specified as arguments.\n",
    "\n",
    "**Note**: for custom agent implementations the Store and tools can be configured to run as needed for any workflow following this pattern. Pre/post model hooks can be used, the whole conversation could be saved at the end, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = create_react_agent(\n",
    "    llm,\n",
    "    store=store,\n",
    "    tools=[],  # No additional tools needed for this example\n",
    "    checkpointer=InMemorySaver(),  # For conversation state management\n",
    "    pre_model_hook=pre_model_hook,  # Saves user message before LLM call\n",
    "    post_model_hook=post_model_hook,  # Saves assistant response for episodic processing after LLM call\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Configure Agent Runtime\n",
    "\n",
    "We need to configure the agent with unique identifiers for the user and session. These IDs are crucial for memory organization and retrieval.\n",
    "\n",
    "### Graph Invoke Input\n",
    "We only need to pass the newest user message in as an argument `inputs`. This could include other state variables as well but for the simple `create_react_agent`, we only need messages.\n",
    "\n",
    "### LangGraph RuntimeConfig\n",
    "In LangGraph, config is a `RuntimeConfig` that contains attributes that are necessary at invocation time, for example user IDs or session IDs. For the `AgentCoreMemorySaver`, `thread_id` and `actor_id` must be set in the config. For instance, your AgentCore invocation endpoint could assign this based on the identity or user ID of the caller. You can read additional [documentation here](https://langchain-ai.github.io/langgraphjs/how-tos/configuration/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_id = \"user-1\"\n",
    "config = {\n",
    "    \"configurable\": {\n",
    "        \"thread_id\": \"session-1\",  # REQUIRED: This maps to Bedrock AgentCore session_id under the hood\n",
    "        \"actor_id\": actor_id,  # REQUIRED: This maps to Bedrock AgentCore actor_id under the hood\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Test the Agent\n",
    "\n",
    "Let's test our nutrition assistant by having a conversation about food preferences. The agent will automatically capture the conversation as episodes for future recall and pattern analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to pretty print agent output while running\n",
    "def run_agent(query: str, config: RunnableConfig):\n",
    "    printed_ids = set()\n",
    "    events = graph.stream(\n",
    "        {\"messages\": [{\"role\": \"user\", \"content\": query}]},\n",
    "        config,\n",
    "        stream_mode=\"values\",\n",
    "    )\n",
    "    for event in events:\n",
    "        if \"messages\" in event:\n",
    "            for msg in event[\"messages\"]:\n",
    "                # Check if we've already printed this message\n",
    "                if id(msg) not in printed_ids:\n",
    "                    msg.pretty_print()\n",
    "                    printed_ids.add(id(msg))\n",
    "\n",
    "\n",
    "prompt = \"\"\"\n",
    "Hey there! Im cooking one of my favorite meals tonight, salmon with rice and veggies (healthy). Has\n",
    "great macros for my weightlifting competition that is coming up. What can I add to this dish to make it taste better\n",
    "and also improve the protein and vitamins I get?\n",
    "\"\"\"\n",
    "\n",
    "run_agent(prompt, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What was stored?\n",
    "As you can see, the model does not yet have any insights from previous meal planning sessions.\n",
    "\n",
    "For this implementation with pre/post model hooks, two messages were stored here. The first message from the user and the response from the AI model were both stored as conversational events in AgentCore Memory. It may take a few moments for the episodes and reflections to be generated, so retry after a few mins if nothing is found the first try.\n",
    "\n",
    "These messages were then processed by the episodic strategy to create structured episodes and reflections in AgentCore long term memory. In fact, we can check the store ourselves to verify what has been stored there so far:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Search our conversation messages\n",
    "search_namespace = (\"nutrition\", actor_id, \"session-1\")\n",
    "result = store.search(search_namespace, query=\"meal\", limit=3)\n",
    "print(f\"Conversation messages result: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The correct way to search episodic long-term memories in LangGraph\n",
    "from bedrock_agentcore.memory import MemoryClient\n",
    "\n",
    "# Use the memory client directly (not the store)\n",
    "memory_client = MemoryClient(region_name=region)\n",
    "\n",
    "print(\"=== Searching Long-Term Episodic Memories ===\")\n",
    "print(f\"Memory ID: {memory_id}\")\n",
    "print()\n",
    "\n",
    "# Search episodic memories (episodes)\n",
    "print(\"1. Episodic namespace: nutrition/user-1/session-1\")\n",
    "try:\n",
    "    episodes = memory_client.retrieve_memories(\n",
    "        memory_id=memory_id,\n",
    "        namespace=\"nutrition/user-1/session-1\",\n",
    "        query=\"meal\",\n",
    "        top_k=3,\n",
    "    )\n",
    "    print(f\"   Found {len(episodes)} episode memories\")\n",
    "    for mem in episodes:\n",
    "        content = mem.get(\"content\", {})\n",
    "        text = content.get(\"text\", str(content))\n",
    "        print(f\"   - {text[:300]}...\")\n",
    "except Exception as e:\n",
    "    print(f\"   Error: {e}\")\n",
    "print()\n",
    "\n",
    "# Search reflection memories\n",
    "print(\"2. Reflection namespace: nutrition/user-1\")\n",
    "try:\n",
    "    reflections = memory_client.retrieve_memories(\n",
    "        memory_id=memory_id, namespace=\"nutrition/user-1\", query=\"meal\", top_k=3\n",
    "    )\n",
    "    print(f\"   Found {len(reflections)} reflection memories\")\n",
    "    for mem in reflections:\n",
    "        content = mem.get(\"content\", {})\n",
    "        text = content.get(\"text\", str(content))\n",
    "        print(f\"   - {text[:300]}...\")\n",
    "except Exception as e:\n",
    "    print(f\"   Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent access to the store\n",
    "\n",
    "**Note** - since AgentCore memory processes these events in the background, it may take a few mins for the memory to be extracted and embedded to long term memory retrieval.\n",
    "\n",
    "Great! Now we have seen that long term memories were extracted to our namespaces based on the earlier messages in the conversation.\n",
    "\n",
    "Now, let's start a new session and ask about recommendations for what to cook for dinner. The agent can use the store to access the long term memories that were extracted to make a recommendation that the user will be sure to like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"configurable\": {\n",
    "        \"thread_id\": \"session-2\",  # New session ID\n",
    "        \"actor_id\": actor_id,  # Same actor ID\n",
    "    }\n",
    "}\n",
    "\n",
    "run_agent(\"Today's a new day, what should I make for dinner tonight?\", config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrapping up\n",
    "\n",
    "As you can see, the agent's conversations are automatically captured and processed into structured episodes with turn-by-turn analysis. The episodic strategy generates insights across multiple meal planning sessions to identify patterns and track how preferences evolve over time.\n",
    "\n",
    "The AgentCoreMemoryStore is very flexible and can be implemented in a variety of ways, including pre/post model hooks or just tools themselves with store operations. Used alongside the AgentCoreMemorySaver for checkpointing, both full conversational state and episodic reflections can be combined to form a complex and intelligent agent system."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

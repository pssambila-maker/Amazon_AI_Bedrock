{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AgentCore Online Evaluation with Actor Simulator\n",
    "\n",
    "**Pipeline:** AgentCore Online Eval Setup → DatasetGenerator → ActorSimulator → Agent Invocation → AgentCore Evaluates via CloudWatch\n",
    "\n",
    "This notebook:\n",
    "1. Loads configuration from `eval_config.py`\n",
    "2. Sets up AgentCore online evaluation with builtin metrics\n",
    "3. Generates test cases using DatasetGenerator\n",
    "4. Runs actor simulator to invoke agent with multi-turn conversations\n",
    "5. AgentCore automatically captures and evaluates traces via CloudWatch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import os\n",
    "from strands_evals import ActorSimulator, Case\n",
    "from strands_evals.generators import DatasetGenerator\n",
    "from strands_eval_config import *\n",
    "\n",
    "os.environ['AWS_DEFAULT_REGION'] = AWS_REGION\n",
    "print(\"Configuration loaded from eval_config.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. AgentCore Online Evaluation Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_client = boto3.client(\n",
    "    'agentcore-evaluation-controlplane',\n",
    "    region_name=AWS_REGION,\n",
    ")\n",
    "\n",
    "create_config_response = evaluation_client.create_online_evaluation_config(\n",
    "    onlineEvaluationConfigName=EVAL_CONFIG_NAME+'_3',\n",
    "    description=EVAL_DESCRIPTION,\n",
    "    rule={\n",
    "        \"samplingConfig\": {\"samplingPercentage\": SAMPLING_PERCENTAGE},\n",
    "        \"sessionConfig\": {\"sessionTimeoutMinutes\": SESSION_TIMEOUT_MINUTES}\n",
    "    },\n",
    "    dataSourceConfig={\n",
    "        \"cloudWatchLogs\": {\n",
    "            \"logGroupNames\": [LOG_GROUP_NAME],\n",
    "            \"serviceNames\": [SERVICE_NAME]\n",
    "        }\n",
    "    },\n",
    "    evaluators=[{\"evaluatorId\": evaluator_id} for evaluator_id in EVALUATORS],\n",
    "    evaluationExecutionRoleArn=EVALUATION_ROLE_ARN,\n",
    "    enableOnCreate=True\n",
    ")\n",
    "\n",
    "config_id = create_config_response['onlineEvaluationConfigId']\n",
    "config_details = evaluation_client.get_online_evaluation_config(onlineEvaluationConfigId=config_id)\n",
    "\n",
    "print(f\"Created config: {config_id}\")\n",
    "print(f\"Status: {config_details['status']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. AgentCore Runtime Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agentcore_client = boto3.client('bedrock-agentcore', region_name=AWS_REGION)\n",
    "\n",
    "def invoke_agentcore(user_message):\n",
    "    boto3_response = agentcore_client.invoke_agent_runtime(\n",
    "        agentRuntimeArn=AGENT_ARN,\n",
    "        qualifier=QUALIFIER,\n",
    "        payload=json.dumps({\"prompt\": user_message})\n",
    "    )\n",
    "    \n",
    "    content = []\n",
    "    if \"text/event-stream\" in boto3_response.get(\"contentType\", \"\"):\n",
    "        for line in boto3_response[\"response\"].iter_lines(chunk_size=1):\n",
    "            if line:\n",
    "                line = line.decode(\"utf-8\")\n",
    "                if line.startswith(\"data: \"):\n",
    "                    line = line[6:]\n",
    "                    content.append(line)\n",
    "    else:\n",
    "        events = []\n",
    "        for event in boto3_response.get(\"response\", []):\n",
    "            events.append(event)\n",
    "        if events:\n",
    "            content.append(json.loads(events[0].decode(\"utf-8\")))\n",
    "    \n",
    "    return \"\\n\".join(str(c) for c in content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Generate Test Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = DatasetGenerator[str, str](str, str)\n",
    "\n",
    "task_description = f\"\"\"\n",
    "Task: {AGENT_CAPABILITIES}\n",
    "Limitations: {AGENT_LIMITATIONS}\n",
    "Available tools: {', '.join(AGENT_TOOLS)}\n",
    "Complexity: {AGENT_COMPLEXITY}\n",
    "\"\"\"\n",
    "\n",
    "dataset = await generator.from_scratch_async(\n",
    "    topics=AGENT_TOPICS,\n",
    "    task_description=task_description,\n",
    "    num_cases=NUM_TEST_CASES\n",
    ")\n",
    "\n",
    "print(f\"Generated {len(dataset.cases)} test cases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Preview Test Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, case in enumerate(dataset.cases, 1):\n",
    "    print(f\"\\nCase {i}: {case.input}\")\n",
    "    print(f\"Expected: {case.expected_output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Define Task Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def task_function(case: Case) -> str:\n",
    "    user_sim = ActorSimulator.from_case_for_user_simulator(case=case, max_turns=MAX_TURNS)\n",
    "    \n",
    "    user_message = case.input\n",
    "    final_response = \"\"\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Test Case: {case.input}\")\n",
    "    print(f\"Expected: {case.expected_output}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    turn = 1\n",
    "    while user_sim.has_next():\n",
    "        print(f\"\\nTurn {turn}: {user_message}\")\n",
    "        agent_response = invoke_agentcore(user_message)\n",
    "        final_response = agent_response\n",
    "        print(f\"Agent: {agent_response[:200]}...\")\n",
    "        \n",
    "        user_result = user_sim.act(agent_response)\n",
    "        user_message = str(user_result.structured_output.message)\n",
    "        turn += 1\n",
    "    \n",
    "    return final_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Run Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for i, case in enumerate(dataset.cases, 1):\n",
    "    print(f\"\\n\\n{'#'*80}\")\n",
    "    print(f\"# Running Test Case {i}/{len(dataset.cases)}\")\n",
    "    print(f\"{'#'*80}\")\n",
    "    \n",
    "    try:\n",
    "        response = task_function(case)\n",
    "        results.append({\n",
    "            \"case_number\": i,\n",
    "            \"input\": case.input,\n",
    "            \"expected\": case.expected_output,\n",
    "            \"actual\": response,\n",
    "            \"status\": \"success\"\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: {e}\")\n",
    "        results.append({\n",
    "            \"case_number\": i,\n",
    "            \"input\": case.input,\n",
    "            \"expected\": case.expected_output,\n",
    "            \"actual\": str(e),\n",
    "            \"status\": \"error\"\n",
    "        })\n",
    "\n",
    "print(f\"\\n\\nCompleted {len(results)} test cases\")\n",
    "print(f\"Successful: {sum(1 for r in results if r['status'] == 'success')}\")\n",
    "print(f\"Errors: {sum(1 for r in results if r['status'] == 'error')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. View Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AgentCore Online Evaluation\n",
    "\n",
    "Agent invocation and evaluation workflow for programmatic testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from IPython.display import Markdown, display\n",
    "from utils import (\n",
    "    EvaluationClient,\n",
    "    generate_session_id,\n",
    "    invoke_and_evaluate,\n",
    ")\n",
    "\n",
    "# Set your AWS Credentials\n",
    "\n",
    "# os.environ['AWS_DEFAULT_REGION'] = 'us-east-1'\n",
    "# os.environ['AWS_ACCESS_KEY_ID'] = ''\n",
    "# os.environ['AWS_SECRET_ACCESS_KEY'] = ''\n",
    "# os.environ['AWS_SESSION_TOKEN'] = ''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AGENT_ID = \"strands_claude_eval\"\n",
    "AGENT_ARN = f\"arn:aws:bedrock-agentcore:us-east-1:<YOUR_ACCOUNT_ID>:runtime/{AGENT_ID}\"\n",
    "REGION = \"us-east-1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_NAME = \"my_experiment_v1\"\n",
    "\n",
    "# Set to None to run ALL 13 evaluators (comprehensive mode)\n",
    "# Or set to a specific list like FLEXIBLE_EVALUATORS to run only those\n",
    "EXPERIMENT_EVALUATORS = None  # Runs all 13 evaluators\n",
    "\n",
    "EXPERIMENT_SCOPE = \"session\"  # Ignored when EXPERIMENT_EVALUATORS = None\n",
    "EXPERIMENT_DELAY = 120 # atleast 120 seconds for the traces to hit AgentCore observability\n",
    "\n",
    "planned_session = generate_session_id() \n",
    "\n",
    "# metadata is optional but you would appreciate it for tracking\n",
    "EXPERIMENT_PROMPTS = [\n",
    "    {\"prompt\": \"What is 2 + 2?\", \"session_id\": \"\", \"metadata\": {\"category\": \"math\"}},\n",
    "    {\"prompt\": \"What is the capital of France?\", \"session_id\": \"\", \"metadata\": {\"category\": \"geography\"}},\n",
    "    {\"prompt\": \"Tell me about quantum physics\", \"session_id\": \"\", \"metadata\": {\"category\": \"science\"}},\n",
    "    {\"prompt\": \"Hello, can you help me with math?\", \"session_id\": planned_session, \"metadata\": {\"turn\": 1}},\n",
    "    {\"prompt\": \"What is 15 * 23?\", \"session_id\": planned_session, \"metadata\": {\"turn\": 2}},\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize AgentCore client\n",
    "agentcore_client = boto3.client('bedrock-agentcore', region_name=REGION)\n",
    "\n",
    "# Initialize Evaluation client\n",
    "eval_client = EvaluationClient(\n",
    "    region=REGION,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FLEXIBLE_EVALUATORS = [\n",
    "    \"Builtin.Correctness\",\n",
    "    \"Builtin.Faithfulness\",\n",
    "    \"Builtin.Helpfulness\",\n",
    "    \"Builtin.ResponseRelevance\",\n",
    "    \"Builtin.Conciseness\",\n",
    "    \"Builtin.Coherence\",\n",
    "    \"Builtin.InstructionFollowing\",\n",
    "    \"Builtin.Refusal\",\n",
    "    \"Builtin.Harmfulness\",\n",
    "    \"Builtin.Stereotyping\"\n",
    "]\n",
    "\n",
    "SESSION_ONLY_EVALUATORS = [\"Builtin.GoalSuccessRate\"]\n",
    "\n",
    "SPAN_ONLY_EVALUATORS = [\n",
    "    \"Builtin.ToolSelectionAccuracy\",\n",
    "    \"Builtin.ToolParameterAccuracy\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_results = []\n",
    "\n",
    "eval_count = len(EXPERIMENT_EVALUATORS) if EXPERIMENT_EVALUATORS else 13\n",
    "print(f\"Experiment: {EXPERIMENT_NAME} | Prompts: {len(EXPERIMENT_PROMPTS)} | Evaluators: {eval_count}\\n\")\n",
    "\n",
    "for i, config in enumerate(EXPERIMENT_PROMPTS, 1):\n",
    "    prompt_text = config[\"prompt\"]\n",
    "    session_id = config.get(\"session_id\", \"\")\n",
    "    metadata = config.get(\"metadata\", {})\n",
    "        \n",
    "    try:\n",
    "        returned_session_id, content, results = invoke_and_evaluate(\n",
    "            agentcore_client=agentcore_client,\n",
    "            eval_client=eval_client,\n",
    "            agent_arn=AGENT_ARN,\n",
    "            agent_id=AGENT_ID,\n",
    "            region=REGION,\n",
    "            prompt=prompt_text,\n",
    "            experiment_name=EXPERIMENT_NAME,\n",
    "            session_id=session_id,\n",
    "            metadata=metadata,\n",
    "            evaluators=EXPERIMENT_EVALUATORS,\n",
    "            scope=EXPERIMENT_SCOPE,\n",
    "            delay=EXPERIMENT_DELAY,\n",
    "            flexible_evaluators=FLEXIBLE_EVALUATORS,\n",
    "            session_only_evaluators=SESSION_ONLY_EVALUATORS,\n",
    "            span_only_evaluators=SPAN_ONLY_EVALUATORS\n",
    "        )\n",
    "        \n",
    "        if content:\n",
    "            display(Markdown(str(content[0])))\n",
    "        \n",
    "        batch_results.append({\n",
    "            \"session_id\": returned_session_id,\n",
    "            \"prompt\": prompt_text,\n",
    "            \"results\": results\n",
    "        })\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\\n\")\n",
    "        batch_results.append({\"prompt\": prompt_text, \"error\": str(e)})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

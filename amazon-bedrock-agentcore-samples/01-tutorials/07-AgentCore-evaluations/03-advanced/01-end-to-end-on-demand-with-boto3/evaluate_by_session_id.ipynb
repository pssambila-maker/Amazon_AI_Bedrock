{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# AgentCore Evaluation - Example Usage\n",
        "\n",
        "This notebook demonstrates helps you evaluate any Sessionid from AgentCore Observability that you want to evaluate."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from utils import EvaluationClient\n",
        "import json\n",
        "\n",
        "# AWS Credentials - Add your credentials here\n",
        "os.environ['AWS_DEFAULT_REGION'] = 'us-east-1'\n",
        "# os.environ['AWS_ACCESS_KEY_ID'] = ''\n",
        "# os.environ['AWS_SECRET_ACCESS_KEY'] = ''\n",
        "# os.environ['AWS_SESSION_TOKEN'] = '' "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# AWS Configuration\n",
        "REGION = \"us-east-1\"\n",
        "AGENT_ID = \"strands_claude_eval-YOUR_UNIQUE_ID\"\n",
        "SESSION_ID = \"<YOUR_SESSION_ID_HERE>\" # pass the session id directly to evaluate \n",
        "\n",
        "metadata = {\n",
        "        \"experiment\": \"evaluation_test\",\n",
        "        \"description\": \"Testing all evaluator scopes\"\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Initialize Client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize evaluation client\n",
        "eval_client = EvaluationClient(\n",
        "    region=REGION,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run Evaluations\n",
        "\n",
        "Evaluates session across all evaluator types and automatically generates dashboard."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluator Groups\n",
        "FLEXIBLE_EVALUATORS = [\n",
        "    \"Builtin.Correctness\",\n",
        "    \"Builtin.Faithfulness\",\n",
        "    \"Builtin.Helpfulness\",\n",
        "    \"Builtin.ResponseRelevance\",\n",
        "    \"Builtin.Conciseness\",\n",
        "    \"Builtin.Coherence\",\n",
        "    \"Builtin.InstructionFollowing\",\n",
        "    \"Builtin.Refusal\",\n",
        "    \"Builtin.Harmfulness\",\n",
        "    \"Builtin.Stereotyping\"\n",
        "]\n",
        "\n",
        "SESSION_ONLY_EVALUATORS = [\"Builtin.GoalSuccessRate\"]\n",
        "\n",
        "SPAN_ONLY_EVALUATORS = [\n",
        "    \"Builtin.ToolSelectionAccuracy\",\n",
        "    \"Builtin.ToolParameterAccuracy\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_groups = [\n",
        "    {\n",
        "        \"name\": \"Flexible Evaluators (session scope)\",\n",
        "        \"evaluators\": FLEXIBLE_EVALUATORS,\n",
        "        \"scope\": \"session\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Session-Only Evaluators\",\n",
        "        \"evaluators\": SESSION_ONLY_EVALUATORS,\n",
        "        \"scope\": \"session\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Span-Only Evaluators\",\n",
        "        \"evaluators\": SPAN_ONLY_EVALUATORS,\n",
        "        \"scope\": \"span\"\n",
        "    }\n",
        "]\n",
        "\n",
        "all_results = []\n",
        "\n",
        "for group in test_groups:\n",
        "    try:\n",
        "        results = eval_client.evaluate_session(\n",
        "            session_id=SESSION_ID,\n",
        "            evaluator_ids=group['evaluators'],\n",
        "            agent_id=AGENT_ID,\n",
        "            region=REGION,\n",
        "            scope=group['scope'],\n",
        "            auto_save_output=True,\n",
        "            auto_create_dashboard=True,\n",
        "            metadata=metadata\n",
        "        )\n",
        "        \n",
        "        print(f\"\\nCompleted: {len(results.results)} evaluations\")\n",
        "        for r in results.results:\n",
        "            print(f\"  {r.evaluator_name}: {r.value} - {r.label}\")\n",
        "            all_results.append(r)\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"\\nError: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# End"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}

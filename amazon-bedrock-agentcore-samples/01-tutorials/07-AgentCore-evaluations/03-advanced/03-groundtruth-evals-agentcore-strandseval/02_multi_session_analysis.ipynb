{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Multi-Session Evaluation\n\nThis notebook evaluates agent sessions using Strands Evals, an extensible LLM-based evaluation framework that uses LLMs as judges. For each session, it fetches traces from AgentCore Observability, runs evaluators, and logs results back with original trace IDs for dashboard correlation.\n\n**This notebook demonstrates two evaluators:**\n- **OutputEvaluator**: Scores response quality (relevance, accuracy, completeness)\n- **TrajectoryEvaluator**: Scores tool usage (selection, efficiency, sequence)\n\nStrands Evals supports custom evaluators for virtually any evaluation type. The framework's power lies in the rubric systemâ€”define your criteria, and the LLM applies them consistently.\n\n**Workflow:**\n1. Load sessions from discovery notebook (or provide custom session IDs)\n2. For each session: fetch traces, create evaluation cases, run evaluators\n3. Log results to AgentCore in EMF format\n4. Generate summary statistics\n\n**Prerequisite:** Run the session discovery notebook first, or prepare a list of session IDs."
  },
  {
   "cell_type": "markdown",
   "source": "## Where This Fits\n\nThis is **Notebook 2 (Option A)** - evaluate sessions using custom rubrics you define.\n\n![Notebook Workflow](images/notebook_workflow.svg)\n\n## How Data Flows\n\nThe evaluation pipeline transforms AgentCore Observability traces into scored results:\n\n![Evaluation Pipeline](images/evaluation_pipeline.svg)",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Setup\n\nImport required modules including Strands Evals evaluators and utility classes for AgentCore Observability interaction. Configuration is loaded from `config.py`."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "from datetime import datetime, timedelta, timezone\n",
    "from typing import List\n",
    "\n",
    "sys.path.insert(0, \".\")\n",
    "\n",
    "from config import (\n",
    "    AWS_REGION,\n",
    "    AWS_ACCOUNT_ID,\n",
    "    SOURCE_LOG_GROUP,\n",
    "    EVAL_RESULTS_LOG_GROUP,\n",
    "    LOOKBACK_HOURS,\n",
    "    MAX_CASES_PER_SESSION,\n",
    "    DISCOVERED_SESSIONS_PATH,\n",
    "    RESULTS_JSON_PATH,\n",
    "    EVALUATION_CONFIG_ID,\n",
    "    setup_cloudwatch_environment,\n",
    ")\n",
    "\n",
    "from utils import (\n",
    "    CloudWatchSessionMapper,\n",
    "    ObservabilityClient,\n",
    "    SessionDiscoveryResult,\n",
    "    SessionInfo,\n",
    "    send_evaluation_to_cloudwatch,\n",
    ")\n",
    "\n",
    "from strands_evals import Case, Experiment\n",
    "from strands_evals.evaluators import OutputEvaluator, TrajectoryEvaluator\n",
    "from strands_evals.types.trace import AgentInvocationSpan, ToolExecutionSpan\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Configuration\n\nDefine evaluator names for CloudWatch metrics. These names appear in the AgentCore Observability dashboard and should follow the `Custom.YourEvaluatorName` convention. The `EVALUATION_CONFIG_ID` is loaded from `config.py`."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom evaluator names for CloudWatch metrics (customize for your use case)\n",
    "OUTPUT_EVALUATOR_NAME = \"Custom.OutputEvaluator\"\n",
    "TRAJECTORY_EVALUATOR_NAME = \"Custom.TrajectoryEvaluator\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CloudWatch Environment\n",
    "\n",
    "Configure environment variables required for logging evaluation results. Uses `SERVICE_NAME` from `config.py` for OTEL resource attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_cloudwatch_environment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Sessions\n",
    "\n",
    "Load sessions from the discovery notebook JSON output. Alternatively, set `USE_JSON_FILE = False` and provide custom session IDs directly for targeted re-evaluation of specific sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set to False to provide custom session IDs instead\n",
    "USE_JSON_FILE = True\n",
    "\n",
    "if USE_JSON_FILE:\n",
    "    discovery_result = SessionDiscoveryResult.load_from_json(DISCOVERED_SESSIONS_PATH)\n",
    "    sessions_to_process = discovery_result.sessions\n",
    "else:\n",
    "    # Provide custom session IDs here\n",
    "    session_ids = [\n",
    "        \"your-session-id-here\",\n",
    "    ]\n",
    "    sessions_to_process = [\n",
    "        SessionInfo(\n",
    "            session_id=sid,\n",
    "            span_count=0,\n",
    "            first_seen=datetime.now(timezone.utc),\n",
    "            last_seen=datetime.now(timezone.utc),\n",
    "            discovery_method=\"user_provided\",\n",
    "        )\n",
    "        for sid in session_ids\n",
    "    ]\n",
    "\n",
    "print(f\"Loaded {len(sessions_to_process)} sessions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluator Rubrics\n",
    "\n",
    "Rubrics define your evaluation criteria. The evaluator sends the rubric along with the agent's output to an LLM, which acts as a judge and returns a score (0.0-1.0) with an explanation.\n",
    "\n",
    "**Writing effective rubrics:**\n",
    "- Be specific about what constitutes good vs poor quality\n",
    "- Include scoring anchors (what does 1.0 vs 0.5 vs 0.0 mean?)\n",
    "- Focus on measurable criteria relevant to your agent's domain\n",
    "\n",
    "Customize these rubrics below. The default rubrics evaluate general response quality and tool usage patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_rubric = \"\"\"\n",
    "Evaluate the agent's response based on:\n",
    "1. Relevance: Does the response directly address the user's question?\n",
    "2. Accuracy: Is the information factually correct?\n",
    "3. Completeness: Does the response provide sufficient detail?\n",
    "\n",
    "Score 0.0-1.0: 1.0=excellent, 0.5=adequate, 0.0=poor\n",
    "\"\"\"\n",
    "\n",
    "trajectory_rubric = \"\"\"\n",
    "Evaluate the agent's tool usage based on:\n",
    "1. Tool Selection: Did the agent choose appropriate tools?\n",
    "2. Efficiency: Were tools used without unnecessary calls?\n",
    "3. Logical Sequence: Were tools used in a logical order?\n",
    "\n",
    "Score 0.0-1.0: 1.0=optimal, 0.5=acceptable, 0.0=poor\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Helper Functions\n\nThese functions bridge AgentCore Observability traces and Strands Evals:\n\n- `task_fn(case)`: Returns the agent's actual response for OutputEvaluator to score against the rubric.\n\n- `trajectory_task_fn(case)`: Returns both the response and tool sequence for TrajectoryEvaluator to assess tool usage patterns.\n\n- `create_cases_from_session(session)`: Converts a Strands Eval Session into evaluation Cases. Extracts user prompts from AgentInvocationSpan, tool names from ToolExecutionSpan objects, and preserves the original trace_id for CloudWatch correlation.\n\n- `log_case_result_to_cloudwatch(case, ...)`: Sends evaluation results to AgentCore Observability using the original trace_id, allowing you to see scores alongside the original traces in the dashboard."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def task_fn(case: Case) -> str:\n",
    "    \"\"\"Return actual output from trace metadata.\"\"\"\n",
    "    return (case.metadata.get(\"actual_output\", \"\"))\n",
    "\n",
    "\n",
    "def trajectory_task_fn(case: Case):\n",
    "    \"\"\"Return output and trajectory from trace metadata.\"\"\"\n",
    "    return {\"output\": case.metadata.get(\"actual_output\", \"\"), \"trajectory\": case.metadata.get(\"trajectory_for_eval\", [])}\n",
    "\n",
    "def log_case_result_to_cloudwatch(case: Case, evaluator_name: str, score: float, explanation: str, label: str = None) -> bool:\n",
    "    \"\"\"Log evaluation result to CloudWatch with original trace ID.\"\"\"\n",
    "    trace_id = case.metadata.get(\"trace_id\", \"\")\n",
    "    if not trace_id:\n",
    "        return False\n",
    "    return send_evaluation_to_cloudwatch(\n",
    "        trace_id=trace_id,\n",
    "        session_id=case.session_id,\n",
    "        evaluator_name=evaluator_name,\n",
    "        score=score,\n",
    "        explanation=explanation,\n",
    "        label=label,\n",
    "        config_id=EVALUATION_CONFIG_ID,\n",
    "    )\n",
    "\n",
    "\n",
    "def create_cases_from_session(session, session_id: str, max_cases: int = None) -> List[Case]:\n",
    "    \"\"\"Create evaluation cases from a Strands Eval Session.\"\"\"\n",
    "    cases = []\n",
    "    for i, trace in enumerate(session.traces):\n",
    "        if max_cases and len(cases) >= max_cases:\n",
    "            break\n",
    "        agent_span = None\n",
    "        tool_names = []\n",
    "        for span in trace.spans:\n",
    "            if isinstance(span, AgentInvocationSpan):\n",
    "                agent_span = span\n",
    "            elif isinstance(span, ToolExecutionSpan):\n",
    "                tool_names.append(span.tool_call.name)\n",
    "        if agent_span:\n",
    "            case = Case(\n",
    "                name=f\"trace_{i+1}_{trace.trace_id[:8]}\",\n",
    "                input=agent_span.user_prompt or \"\",\n",
    "                expected_output=\"\",\n",
    "                session_id=session_id,\n",
    "                metadata={\n",
    "                    \"actual_output\": agent_span.agent_response or \"\",\n",
    "                    \"actual_trajectory\": tool_names,\n",
    "                    \"trace_id\": trace.trace_id,\n",
    "                    \"tool_count\": len(tool_names),\n",
    "                },\n",
    "            )\n",
    "            cases.append(case)\n",
    "    return cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Initialize Client\n\nCreate the `ObservabilityClient` to fetch traces and the `CloudWatchSessionMapper` to convert them.\n\nThe mapper transforms raw AgentCore Observability spans into structured Strands Eval objects:\n- Groups spans by trace_id to reconstruct each interaction\n- Extracts tool calls and matches them with their results\n- Identifies user prompts (first message) and agent responses (final output)\n- Produces AgentInvocationSpan (full interaction) and ToolExecutionSpan (each tool use)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_client = ObservabilityClient(\n",
    "    region_name=AWS_REGION,\n",
    "    log_group=SOURCE_LOG_GROUP,\n",
    ")\n",
    "mapper = CloudWatchSessionMapper()\n",
    "\n",
    "end_time = datetime.now(timezone.utc)\n",
    "start_time = end_time - timedelta(hours=LOOKBACK_HOURS)\n",
    "start_time_ms = int(start_time.timestamp() * 1000)\n",
    "end_time_ms = int(end_time.timestamp() * 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Process Sessions\n\nThe main evaluation loop. For each session:\n1. Fetch spans from AgentCore Observability\n2. Convert spans to Strands Eval Session format using the mapper\n3. Create evaluation Cases from each trace in the session\n4. Run OutputEvaluator on all cases\n5. Run TrajectoryEvaluator on cases that used tools\n6. Log all results to AgentCore Observability with original trace IDs for dashboard correlation\n\nProgress is printed for each session. Errors are caught and logged without stopping the loop."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_session_results = []\n",
    "total_cases_evaluated = 0\n",
    "total_logs_sent = 0\n",
    "all_tools_used = set()\n",
    "\n",
    "for session_idx, session_info in enumerate(sessions_to_process):\n",
    "    session_id = session_info.session_id\n",
    "    print(f\"[{session_idx + 1}/{len(sessions_to_process)}] {session_id}\")\n",
    "\n",
    "    try:\n",
    "        trace_data = obs_client.get_session_data(\n",
    "            session_id=session_id,\n",
    "            start_time_ms=start_time_ms,\n",
    "            end_time_ms=end_time_ms,\n",
    "            include_runtime_logs=False,\n",
    "        )\n",
    "\n",
    "        if not trace_data.spans:\n",
    "            all_session_results.append({\"session_id\": session_id, \"status\": \"skipped\", \"reason\": \"no_spans\"})\n",
    "            continue\n",
    "\n",
    "        session = trace_data.to_session(mapper)\n",
    "        cases = create_cases_from_session(session, session_id, MAX_CASES_PER_SESSION)\n",
    "\n",
    "        if not cases:\n",
    "            all_session_results.append({\"session_id\": session_id, \"status\": \"skipped\", \"reason\": \"no_cases\"})\n",
    "            continue\n",
    "\n",
    "        for case in cases:\n",
    "            for tool in case.metadata.get(\"actual_trajectory\", []):\n",
    "                all_tools_used.add(tool)\n",
    "\n",
    "        # Run Output Evaluator\n",
    "        output_experiment = Experiment(cases=cases, evaluators=[OutputEvaluator(rubric=output_rubric)])\n",
    "        output_results = output_experiment.run_evaluations(task_fn)\n",
    "        output_report = output_results[0]\n",
    "\n",
    "        output_logged = 0\n",
    "        for i, case in enumerate(cases):\n",
    "            if log_case_result_to_cloudwatch(case, OUTPUT_EVALUATOR_NAME, output_report.scores[i], output_report.reasons[i] if i < len(output_report.reasons) else \"\"):\n",
    "                output_logged += 1\n",
    "\n",
    "        # Run Trajectory Evaluator\n",
    "        trajectory_cases = [c for c in cases if c.metadata.get(\"actual_trajectory\")]\n",
    "        trajectory_score = None\n",
    "        trajectory_logged = 0\n",
    "\n",
    "        if trajectory_cases:\n",
    "            traj_eval_cases = [\n",
    "                Case(name=c.name, input=c.input, expected_output=c.expected_output, session_id=c.session_id,\n",
    "                     metadata={**c.metadata, \"trajectory_for_eval\": c.metadata.get(\"actual_trajectory\", [])})\n",
    "                for c in trajectory_cases\n",
    "            ]\n",
    "            trajectory_experiment = Experiment(\n",
    "                cases=traj_eval_cases,\n",
    "                evaluators=[TrajectoryEvaluator(rubric=trajectory_rubric, trajectory_description={\"available_tools\": list(all_tools_used)})]\n",
    "            )\n",
    "            trajectory_results = trajectory_experiment.run_evaluations(trajectory_task_fn)\n",
    "            trajectory_report = trajectory_results[0]\n",
    "            trajectory_score = trajectory_report.overall_score\n",
    "\n",
    "            for i, case in enumerate(traj_eval_cases):\n",
    "                if log_case_result_to_cloudwatch(case, TRAJECTORY_EVALUATOR_NAME, trajectory_report.scores[i], trajectory_report.reasons[i] if i < len(trajectory_report.reasons) else \"\"):\n",
    "                    trajectory_logged += 1\n",
    "\n",
    "        all_session_results.append({\n",
    "            \"session_id\": session_id,\n",
    "            \"status\": \"completed\",\n",
    "            \"case_count\": len(cases),\n",
    "            \"output_score\": output_report.overall_score,\n",
    "            \"trajectory_score\": trajectory_score,\n",
    "            \"logs_sent\": output_logged + trajectory_logged,\n",
    "        })\n",
    "        total_cases_evaluated += len(cases)\n",
    "        total_logs_sent += output_logged + trajectory_logged\n",
    "\n",
    "    except Exception as e:\n",
    "        all_session_results.append({\"session_id\": session_id, \"status\": \"error\", \"error\": str(e)})\n",
    "\n",
    "print(f\"\\nCompleted: {len([r for r in all_session_results if r['status'] == 'completed'])} sessions, {total_cases_evaluated} cases, {total_logs_sent} logs sent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Aggregate statistics across all evaluated sessions including completion rate, total cases evaluated, and average scores for both output and trajectory evaluators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completed = [r for r in all_session_results if r.get(\"status\") == \"completed\"]\n",
    "output_scores = [r[\"output_score\"] for r in completed if r.get(\"output_score\") is not None]\n",
    "trajectory_scores = [r[\"trajectory_score\"] for r in completed if r.get(\"trajectory_score\") is not None]\n",
    "\n",
    "print(f\"Sessions: {len(completed)}/{len(all_session_results)} completed\")\n",
    "print(f\"Cases evaluated: {total_cases_evaluated}\")\n",
    "print(f\"CloudWatch logs sent: {total_logs_sent}\")\n",
    "\n",
    "if output_scores:\n",
    "    print(f\"Output score: avg={sum(output_scores)/len(output_scores):.2f}, min={min(output_scores):.2f}, max={max(output_scores):.2f}\")\n",
    "if trajectory_scores:\n",
    "    print(f\"Trajectory score: avg={sum(trajectory_scores)/len(trajectory_scores):.2f}, min={min(trajectory_scores):.2f}, max={max(trajectory_scores):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Per-Session Results\n",
    "\n",
    "Individual results for each session showing output and trajectory scores. Sessions marked \"skipped\" had no spans or valid cases. Sessions marked \"error\" encountered exceptions during processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, r in enumerate(all_session_results):\n",
    "    status = r.get(\"status\", \"unknown\")\n",
    "    if status == \"completed\":\n",
    "        print(f\"{i+1}. {r['session_id'][:20]}... output={r.get('output_score', 0):.2f} traj={r.get('trajectory_score') or '-'}\")\n",
    "    else:\n",
    "        print(f\"{i+1}. {r['session_id'][:20]}... {status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Results\n",
    "\n",
    "Save evaluation results to JSON for further analysis or reporting. The export includes configuration, summary statistics, and per-session results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "export_data = {\n",
    "    \"evaluation_time\": datetime.now(timezone.utc).isoformat(),\n",
    "    \"config\": {\n",
    "        \"source_log_group\": SOURCE_LOG_GROUP,\n",
    "        \"eval_results_log_group\": EVAL_RESULTS_LOG_GROUP,\n",
    "        \"output_evaluator\": OUTPUT_EVALUATOR_NAME,\n",
    "        \"trajectory_evaluator\": TRAJECTORY_EVALUATOR_NAME,\n",
    "    },\n",
    "    \"summary\": {\n",
    "        \"total_sessions\": len(all_session_results),\n",
    "        \"completed_sessions\": len(completed),\n",
    "        \"total_cases\": total_cases_evaluated,\n",
    "        \"avg_output_score\": sum(output_scores) / len(output_scores) if output_scores else None,\n",
    "        \"avg_trajectory_score\": sum(trajectory_scores) / len(trajectory_scores) if trajectory_scores else None,\n",
    "    },\n",
    "    \"session_results\": all_session_results,\n",
    "}\n",
    "\n",
    "with open(RESULTS_JSON_PATH, \"w\") as f:\n",
    "    json.dump(export_data, f, indent=2)\n",
    "\n",
    "print(f\"Exported to {RESULTS_JSON_PATH}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
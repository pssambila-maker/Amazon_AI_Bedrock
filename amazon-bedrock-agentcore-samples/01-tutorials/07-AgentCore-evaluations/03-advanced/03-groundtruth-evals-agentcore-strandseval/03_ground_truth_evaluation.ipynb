{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Ground Truth Evaluation\n\nThis notebook evaluates agent responses against ground truth (expected outputs) using Strands Evals, an extensible LLM-based evaluation framework. Unlike rubric-only evaluation, ground truth evaluation compares actual outputs to predefined correct answers.\n\n**Use Cases:**\n- Regression testing: Ensure agent changes don't break known-good responses\n- Quality benchmarking: Measure how well agents match expected behavior\n- Training data validation: Verify agent outputs against curated examples\n\n**Two Data Sources (Separate Files):**\n1. **Traces File** (`demo_traces.json`): Contains actual agent responses from AgentCore Observability\n2. **Ground Truth File** (`demo_ground_truth.json`): Contains YOUR expected outputs for each trace\n\nThe notebook merges these files by `trace_id` to compare actual vs expected.\n\n**Two Modes:**\n1. **Demo Mode**: Load sample data from JSON files (no AWS access needed)\n2. **Live Mode**: Fetch real traces from AgentCore Observability, provide your own ground truth file\n\n**This notebook demonstrates two evaluators:**\n- Output evaluation: Compares actual response against expected ground truth\n- Trajectory evaluation: Compares actual tool usage against expected tools\n\nStrands Evals supports custom evaluators for virtually any evaluation typeâ€”you can extend this pattern to evaluate any criteria that can be expressed as scoring criteria.\n\n**Workflow:**\n1. Load traces (demo files or live from AgentCore Observability)\n2. Load ground truth expectations (your expected outputs/trajectories)\n3. Merge by trace_id\n4. Run evaluators comparing actual vs expected\n5. Log results to AgentCore Observability (optional)\n6. Analyze results and identify gaps"
  },
  {
   "cell_type": "markdown",
   "source": "## Where This Fits\n\nThis is **Notebook 3 (Option B)** - evaluate sessions by comparing against expected ground truth outputs.\n\n![Notebook Workflow](images/notebook_workflow.svg)\n\n## How Ground Truth Evaluation Works\n\nAn SME (Subject Matter Expert) creates a ground truth file with expected outputs. This is merged with actual traces by `trace_id`:\n\n![Ground Truth Flow](images/ground_truth_flow.svg)",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Import modules and configure logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import sys\n",
    "from datetime import datetime, timedelta, timezone\n",
    "from typing import List, Dict, Any, Optional\n",
    "\n",
    "sys.path.insert(0, \".\")\n",
    "\n",
    "from strands_evals import Case, Experiment\n",
    "from strands_evals.evaluators import OutputEvaluator, TrajectoryEvaluator\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Configuration\n\nChoose between Demo Mode (uses sample files) or Live Mode (fetches from AgentCore Observability).\n\n**Demo Mode** (`USE_DEMO_MODE = True`):\n- Loads traces from `DEMO_TRACES_PATH`\n- Loads ground truth from `DEMO_GROUND_TRUTH_PATH`\n- No AWS credentials required\n\n**Live Mode** (`USE_DEMO_MODE = False`):\n- Fetches traces from AgentCore Observability using `SESSION_ID`\n- Loads ground truth from `GROUND_TRUTH_PATH` (you must create this file)\n- Requires AWS credentials and `config.py` settings\n\n**CloudWatch Logging** (`LOG_TO_CLOUDWATCH = True`):\n- Sends evaluation results to AgentCore Observability dashboard\n- Requires `EVALUATION_CONFIG_ID` and evaluator names"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Mode Selection\n",
    "# =============================================================================\n",
    "USE_DEMO_MODE = True\n",
    "\n",
    "# =============================================================================\n",
    "# Demo Mode Paths\n",
    "# =============================================================================\n",
    "DEMO_TRACES_PATH = \"demo_traces.json\"           # Actual agent responses\n",
    "DEMO_GROUND_TRUTH_PATH = \"demo_ground_truth.json\"  # Your expected outputs\n",
    "\n",
    "# =============================================================================\n",
    "# Live Mode Settings\n",
    "# =============================================================================\n",
    "SESSION_ID = \"your-session-id-here\"              # Session to evaluate\n",
    "GROUND_TRUTH_PATH = \"my_ground_truth.json\"       # Your ground truth file\n",
    "\n",
    "# =============================================================================\n",
    "# CloudWatch Logging\n",
    "# =============================================================================\n",
    "LOG_TO_CLOUDWATCH = True\n",
    "OUTPUT_EVALUATOR_NAME = \"Custom.GroundTruthOutput\"\n",
    "TRAJECTORY_EVALUATOR_NAME = \"Custom.GroundTruthTrajectory\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## File Formats\n\nGround truth evaluation uses **two separate files** that share the same `session_id`:\n\n**Key concepts:**\n- `session_id`: Groups all traces from a single user session\n- `trace_id`: Identifies each individual interaction within the session\n\n### 1. Traces File (actual agent responses)\nContains what the agent actually did - fetched from CloudWatch or saved locally:\n```json\n{\n  \"session_id\": \"5B467129-E54A-4F70-908D-CB31818004B5\",\n  \"traces\": [\n    {\n      \"trace_id\": \"693cb6c4e931\",\n      \"user_prompt\": \"What is the best route for a NZ road trip?\",\n      \"actual_output\": \"Based on the search results, here are the best routes...\",\n      \"actual_trajectory\": [\"web_search\"]\n    },\n    {\n      \"trace_id\": \"693cb6fa87aa\",\n      \"user_prompt\": \"Should I visit North or South Island?\",\n      \"actual_output\": \"Here's how the islands compare...\",\n      \"actual_trajectory\": [\"web_search\"]\n    }\n  ]\n}\n```\n\n### 2. Ground Truth File (your expected outputs)\nSME reviews the traces and writes expected outputs for each `trace_id`:\n```json\n{\n  \"session_id\": \"5B467129-E54A-4F70-908D-CB31818004B5\",\n  \"ground_truth\": [\n    {\n      \"trace_id\": \"693cb6c4e931\",\n      \"user_prompt_reference\": \"What is the best route for a NZ road trip?\",\n      \"expected_output\": \"Response should mention Milford Road, Southern Scenic Route...\",\n      \"expected_trajectory\": [\"web_search\"]\n    },\n    {\n      \"trace_id\": \"693cb6fa87aa\",\n      \"user_prompt_reference\": \"Should I visit North or South Island?\",\n      \"expected_output\": \"Response should compare both islands with key features...\",\n      \"expected_trajectory\": [\"web_search\"]\n    }\n  ]\n}\n```\n\n**Note:** `user_prompt_reference` is optional - it helps SMEs remember which trace they're writing expectations for."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Traces and Ground Truth\n",
    "\n",
    "Load trace data from demo files or CloudWatch, then load ground truth expectations and merge by `trace_id`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_DEMO_MODE:\n",
    "    # Load traces (actual agent responses)\n",
    "    with open(DEMO_TRACES_PATH, \"r\") as f:\n",
    "        traces_data = json.load(f)\n",
    "    \n",
    "    SESSION_ID = traces_data[\"session_id\"]\n",
    "    traces = []\n",
    "    for i, t in enumerate(traces_data[\"traces\"]):\n",
    "        traces.append({\n",
    "            \"trace_index\": i,\n",
    "            \"trace_id\": t.get(\"trace_id\", f\"demo-trace-{i:03d}\"),\n",
    "            \"user_prompt\": t[\"user_prompt\"],\n",
    "            \"actual_output\": t.get(\"actual_output\", \"\"),\n",
    "            \"actual_trajectory\": t.get(\"actual_trajectory\", []),\n",
    "        })\n",
    "    \n",
    "    # Load ground truth (expected outputs) - separate file!\n",
    "    with open(DEMO_GROUND_TRUTH_PATH, \"r\") as f:\n",
    "        gt_data = json.load(f)\n",
    "    \n",
    "    # Build ground truth lookup by trace_id\n",
    "    gt_by_trace_id = {\n",
    "        gt[\"trace_id\"]: {\n",
    "            \"expected_output\": gt[\"expected_output\"],\n",
    "            \"expected_trajectory\": gt.get(\"expected_trajectory\", []),\n",
    "        }\n",
    "        for gt in gt_data[\"ground_truth\"]\n",
    "    }\n",
    "    \n",
    "    # Merge: match traces to ground truth by trace_id\n",
    "    ground_truth = {}\n",
    "    matched_count = 0\n",
    "    for trace in traces:\n",
    "        trace_id = trace[\"trace_id\"]\n",
    "        if trace_id in gt_by_trace_id:\n",
    "            ground_truth[trace[\"trace_index\"]] = gt_by_trace_id[trace_id]\n",
    "            matched_count += 1\n",
    "    \n",
    "    print(f\"Demo Mode:\")\n",
    "    print(f\"  Traces loaded: {len(traces)} from {DEMO_TRACES_PATH}\")\n",
    "    print(f\"  Ground truth loaded: {len(gt_data['ground_truth'])} entries from {DEMO_GROUND_TRUTH_PATH}\")\n",
    "    print(f\"  Matched by trace_id: {matched_count}\")\n",
    "    print(f\"  Session ID: {SESSION_ID}\")\n",
    "    if gt_data.get(\"description\"):\n",
    "        print(f\"  Description: {gt_data['description']}\")\n",
    "\n",
    "else:\n",
    "    from config import AWS_REGION, SOURCE_LOG_GROUP, LOOKBACK_HOURS\n",
    "    from utils import CloudWatchSessionMapper, ObservabilityClient\n",
    "    from strands_evals.types.trace import AgentInvocationSpan, ToolExecutionSpan\n",
    "    \n",
    "    # Fetch traces from CloudWatch\n",
    "    obs_client = ObservabilityClient(region_name=AWS_REGION, log_group=SOURCE_LOG_GROUP)\n",
    "    mapper = CloudWatchSessionMapper()\n",
    "    \n",
    "    end_time = datetime.now(timezone.utc)\n",
    "    start_time = end_time - timedelta(hours=LOOKBACK_HOURS)\n",
    "    \n",
    "    trace_data = obs_client.get_session_data(\n",
    "        session_id=SESSION_ID,\n",
    "        start_time_ms=int(start_time.timestamp() * 1000),\n",
    "        end_time_ms=int(end_time.timestamp() * 1000),\n",
    "        include_runtime_logs=False,\n",
    "    )\n",
    "    \n",
    "    if not trace_data.spans:\n",
    "        raise ValueError(f\"No spans found for session {SESSION_ID}\")\n",
    "    \n",
    "    session = trace_data.to_session(mapper)\n",
    "    traces = []\n",
    "    for i, trace in enumerate(session.traces):\n",
    "        agent_span = None\n",
    "        tool_calls = []\n",
    "        for span in trace.spans:\n",
    "            if isinstance(span, AgentInvocationSpan):\n",
    "                agent_span = span\n",
    "            elif isinstance(span, ToolExecutionSpan):\n",
    "                tool_calls.append(span.tool_call.name)\n",
    "        if agent_span:\n",
    "            traces.append({\n",
    "                \"trace_index\": i,\n",
    "                \"trace_id\": trace.trace_id,\n",
    "                \"user_prompt\": agent_span.user_prompt or \"\",\n",
    "                \"actual_output\": agent_span.agent_response or \"\",\n",
    "                \"actual_trajectory\": tool_calls,\n",
    "            })\n",
    "    \n",
    "    # Load ground truth from separate file\n",
    "    try:\n",
    "        with open(GROUND_TRUTH_PATH, \"r\") as f:\n",
    "            gt_data = json.load(f)\n",
    "        gt_by_trace_id = {\n",
    "            gt[\"trace_id\"]: {\n",
    "                \"expected_output\": gt[\"expected_output\"],\n",
    "                \"expected_trajectory\": gt.get(\"expected_trajectory\", []),\n",
    "            }\n",
    "            for gt in gt_data[\"ground_truth\"]\n",
    "        }\n",
    "        ground_truth = {}\n",
    "        for trace in traces:\n",
    "            trace_id = trace[\"trace_id\"]\n",
    "            if trace_id in gt_by_trace_id:\n",
    "                ground_truth[trace[\"trace_index\"]] = gt_by_trace_id[trace_id]\n",
    "        print(f\"Ground truth loaded: {len(ground_truth)} matches from {GROUND_TRUTH_PATH}\")\n",
    "    except FileNotFoundError:\n",
    "        ground_truth = {}\n",
    "        print(f\"Ground truth file not found: {GROUND_TRUTH_PATH}\")\n",
    "        print(\"Create a ground truth file or define manually in the next section\")\n",
    "    \n",
    "    print(f\"Live Mode: Loaded {len(traces)} traces from CloudWatch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review Traces\n",
    "\n",
    "Display each trace's details. Review these to understand what ground truth should look like for each interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for trace in traces:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"TRACE {trace['trace_index'] + 1} (ID: {trace['trace_id']})\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    prompt = trace['user_prompt']\n",
    "    output = trace['actual_output']\n",
    "    \n",
    "    print(f\"\\nUSER PROMPT:\\n{prompt[:500]}...\" if len(prompt) > 500 else f\"\\nUSER PROMPT:\\n{prompt}\")\n",
    "    print(f\"\\nACTUAL OUTPUT:\\n{output[:500]}...\" if len(output) > 500 else f\"\\nACTUAL OUTPUT:\\n{output}\")\n",
    "    print(f\"\\nACTUAL TRAJECTORY: {trace['actual_trajectory']}\")\n",
    "    \n",
    "    if trace['trace_index'] in ground_truth:\n",
    "        gt = ground_truth[trace['trace_index']]\n",
    "        print(f\"\\nEXPECTED OUTPUT: {gt['expected_output']}\")\n",
    "        print(f\"EXPECTED TRAJECTORY: {gt['expected_trajectory']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Ground Truth (Live Mode Only)\n",
    "\n",
    "If using Live Mode, define the expected output and expected trajectory for each trace here.\n",
    "\n",
    "**In Demo Mode, ground truth is pre-loaded from the JSON file.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not ground_truth:\n",
    "    print(\"Ground truth not defined. Generating template...\\n\")\n",
    "    print(\"Copy, edit, and paste the following:\\n\")\n",
    "    print(\"ground_truth = {\")\n",
    "    for trace in traces:\n",
    "        prompt_preview = trace['user_prompt'][:50].replace('\"', \"'\")\n",
    "        print(f\"    {trace['trace_index']}: {{\")\n",
    "        print(f'        \"expected_output\": \"TODO: {prompt_preview}...\",') \n",
    "        print(f'        \"expected_trajectory\": {trace[\"actual_trajectory\"]},')\n",
    "        print(f\"    }},\")\n",
    "    print(\"}\")\n",
    "else:\n",
    "    print(f\"Ground truth already defined for {len(ground_truth)} traces\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluator Rubrics\n",
    "\n",
    "Both evaluators use rubrics to compare actual vs expected values:\n",
    "\n",
    "- **OutputEvaluator**: Compares actual response against expected output using semantic similarity\n",
    "- **TrajectoryEvaluator**: Compares actual tool trajectory against expected trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_output_rubric = \"\"\"\n",
    "Compare the agent's actual output against the expected ground truth output.\n",
    "\n",
    "Evaluation criteria:\n",
    "1. Semantic Match (0-0.5): Does the actual output convey the same meaning as expected?\n",
    "   - 0.5: Full semantic alignment - same information and intent\n",
    "   - 0.3: Partial alignment - captures main points but misses details\n",
    "   - 0.0: No alignment - different information or wrong answer\n",
    "\n",
    "2. Completeness (0-0.3): Does the actual output include all key points from expected?\n",
    "   - 0.3: All key information present\n",
    "   - 0.15: Most key information present\n",
    "   - 0.0: Missing critical information\n",
    "\n",
    "3. Correctness (0-0.2): Is the actual output factually consistent with expected?\n",
    "   - 0.2: No factual contradictions\n",
    "   - 0.1: Minor inconsistencies\n",
    "   - 0.0: Major contradictions or errors\n",
    "\n",
    "Final score = sum of all criteria (0.0 to 1.0)\n",
    "\"\"\"\n",
    "\n",
    "trajectory_rubric = \"\"\"\n",
    "Compare the agent's actual tool trajectory against the expected trajectory.\n",
    "\n",
    "Evaluation criteria:\n",
    "1. Tool Match (0-0.5): Did the agent use the expected tools?\n",
    "   - 0.5: All expected tools were used\n",
    "   - 0.25: Some expected tools were used\n",
    "   - 0.0: None of the expected tools were used\n",
    "\n",
    "2. No Extra Tools (0-0.3): Did the agent avoid unnecessary tools?\n",
    "   - 0.3: No extra tools beyond expected\n",
    "   - 0.15: One extra tool\n",
    "   - 0.0: Multiple unnecessary tools\n",
    "\n",
    "3. Order (0-0.2): Were tools used in the expected sequence?\n",
    "   - 0.2: Correct order\n",
    "   - 0.1: Minor order differences\n",
    "   - 0.0: Completely different order\n",
    "\n",
    "Final score = sum of all criteria (0.0 to 1.0)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Evaluation Cases\n",
    "\n",
    "Create Cases that include both actual outputs and expected ground truth for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ground_truth_cases(traces: List[Dict], ground_truth: Dict[int, Dict], session_id: str) -> List[Case]:\n",
    "    \"\"\"Create evaluation cases with ground truth for comparison.\"\"\"\n",
    "    cases = []\n",
    "    \n",
    "    for trace in traces:\n",
    "        idx = trace[\"trace_index\"]\n",
    "        gt = ground_truth.get(idx, {})\n",
    "        \n",
    "        if not gt:\n",
    "            print(f\"Warning: No ground truth for trace {idx}, skipping\")\n",
    "            continue\n",
    "        \n",
    "        case = Case(\n",
    "            name=f\"trace_{idx}_{trace['trace_id'][:8]}\",\n",
    "            input=trace[\"user_prompt\"],\n",
    "            expected_output=gt.get(\"expected_output\", \"\"),\n",
    "            session_id=session_id,\n",
    "            metadata={\n",
    "                \"actual_output\": trace[\"actual_output\"],\n",
    "                \"actual_trajectory\": trace[\"actual_trajectory\"],\n",
    "                \"expected_trajectory\": gt.get(\"expected_trajectory\", []),\n",
    "                \"trace_id\": trace[\"trace_id\"],\n",
    "            },\n",
    "        )\n",
    "        cases.append(case)\n",
    "    \n",
    "    return cases\n",
    "\n",
    "cases = create_ground_truth_cases(traces, ground_truth, SESSION_ID)\n",
    "print(f\"Created {len(cases)} evaluation cases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task Functions\n",
    "\n",
    "These functions extract actual and expected values for the evaluator to compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ground_truth_task_fn(case: Case) -> str:\n",
    "    \"\"\"Return actual and expected output for comparison.\"\"\"\n",
    "    actual = case.metadata.get(\"actual_output\", \"\")\n",
    "    expected = case.expected_output or \"\"\n",
    "    return f\"ACTUAL OUTPUT:\\n{actual}\\n\\nEXPECTED OUTPUT (Ground Truth):\\n{expected}\"\n",
    "\n",
    "\n",
    "def trajectory_task_fn(case: Case):\n",
    "    \"\"\"Return output and trajectory for TrajectoryEvaluator.\n",
    "    \n",
    "    TrajectoryEvaluator expects a dictionary with 'output' and 'trajectory' keys.\n",
    "    \"\"\"\n",
    "    actual_output = case.metadata.get(\"actual_output\", \"\")\n",
    "    actual_trajectory = case.metadata.get(\"actual_trajectory\", [])\n",
    "    expected_trajectory = case.metadata.get(\"expected_trajectory\", [])\n",
    "    \n",
    "    # Format output to include comparison context\n",
    "    comparison_output = f\"\"\"ACTUAL OUTPUT:\n",
    "{actual_output}\n",
    "\n",
    "EXPECTED TRAJECTORY: {expected_trajectory}\n",
    "ACTUAL TRAJECTORY: {actual_trajectory}\"\"\"\n",
    "    \n",
    "    # Return dictionary format expected by TrajectoryEvaluator\n",
    "    return {\"output\": comparison_output, \"trajectory\": actual_trajectory}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Ground Truth Evaluation\n",
    "\n",
    "Execute the evaluation comparing actual outputs against ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cases:\n",
    "    print(\"Running Output Evaluation...\")\n",
    "    output_evaluator = OutputEvaluator(rubric=ground_truth_output_rubric)\n",
    "    output_experiment = Experiment(cases=cases, evaluators=[output_evaluator])\n",
    "    output_results = output_experiment.run_evaluations(ground_truth_task_fn)\n",
    "    output_report = output_results[0]  # Extract the report from the list\n",
    "    print(f\"Output Evaluation Complete - Overall Score: {output_report.overall_score:.2f}\")\n",
    "else:\n",
    "    print(\"No cases to evaluate. Please define ground truth first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cases:\n",
    "    print(\"Running Trajectory Evaluation...\")\n",
    "    \n",
    "    # Collect all unique tools from actual and expected trajectories\n",
    "    all_tools = set()\n",
    "    for trace in traces:\n",
    "        all_tools.update(trace[\"actual_trajectory\"])\n",
    "    for gt in ground_truth.values():\n",
    "        all_tools.update(gt.get(\"expected_trajectory\", []))\n",
    "    \n",
    "    trajectory_evaluator = TrajectoryEvaluator(\n",
    "        rubric=trajectory_rubric,\n",
    "        trajectory_description={\"available_tools\": list(all_tools)}\n",
    "    )\n",
    "    trajectory_experiment = Experiment(cases=cases, evaluators=[trajectory_evaluator])\n",
    "    trajectory_results = trajectory_experiment.run_evaluations(trajectory_task_fn)\n",
    "    trajectory_report = trajectory_results[0]\n",
    "    print(f\"Trajectory Evaluation Complete - Overall Score: {trajectory_report.overall_score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detailed Results\n",
    "\n",
    "View per-trace scores and explanations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cases:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"GROUND TRUTH EVALUATION RESULTS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    for i, case in enumerate(cases):\n",
    "        print(f\"\\n--- Trace {i+1}: {case.name} ---\")\n",
    "        prompt_display = case.input[:80] + \"...\" if len(case.input) > 80 else case.input\n",
    "        print(f\"User Prompt: {prompt_display}\")\n",
    "        \n",
    "        output_score = output_report.scores[i] if i < len(output_report.scores) else 0\n",
    "        output_reason = output_report.reasons[i] if i < len(output_report.reasons) else \"N/A\"\n",
    "        print(f\"\\nOutput Score: {output_score:.2f}\")\n",
    "        print(f\"Explanation: {output_reason[:250]}...\" if len(str(output_reason)) > 250 else f\"Explanation: {output_reason}\")\n",
    "        \n",
    "        traj_score = trajectory_report.scores[i] if i < len(trajectory_report.scores) else 0\n",
    "        traj_reason = trajectory_report.reasons[i] if i < len(trajectory_report.reasons) else \"N/A\"\n",
    "        print(f\"\\nTrajectory Score: {traj_score:.2f}\")\n",
    "        print(f\"Expected Tools: {case.metadata.get('expected_trajectory', [])}\")\n",
    "        print(f\"Actual Tools:   {case.metadata.get('actual_trajectory', [])}\")\n",
    "        print(f\"Explanation: {traj_reason[:250]}...\" if len(str(traj_reason)) > 250 else f\"Explanation: {traj_reason}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Log Results to CloudWatch (Optional)\n\nSend evaluation results to AgentCore Observability dashboard using the original trace IDs. This allows you to see ground truth evaluation scores alongside the original traces in the AgentCore Observability console.\n\nSet `LOG_TO_CLOUDWATCH = True` in the configuration section to enable this feature."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOG_TO_CLOUDWATCH and cases:\n",
    "    from config import EVALUATION_CONFIG_ID, setup_cloudwatch_environment\n",
    "    from utils import send_evaluation_to_cloudwatch\n",
    "    \n",
    "    # Setup CloudWatch environment\n",
    "    setup_cloudwatch_environment()\n",
    "    \n",
    "    output_logged = 0\n",
    "    trajectory_logged = 0\n",
    "    \n",
    "    print(\"Logging results to CloudWatch...\")\n",
    "    \n",
    "    for i, case in enumerate(cases):\n",
    "        trace_id = case.metadata.get(\"trace_id\", \"\")\n",
    "        if not trace_id:\n",
    "            continue\n",
    "        \n",
    "        # Log output evaluation result\n",
    "        output_score = output_report.scores[i] if i < len(output_report.scores) else 0\n",
    "        output_reason = output_report.reasons[i] if i < len(output_report.reasons) else \"\"\n",
    "        if send_evaluation_to_cloudwatch(\n",
    "            trace_id=trace_id,\n",
    "            session_id=SESSION_ID,\n",
    "            evaluator_name=OUTPUT_EVALUATOR_NAME,\n",
    "            score=output_score,\n",
    "            explanation=str(output_reason)[:500],\n",
    "            config_id=EVALUATION_CONFIG_ID,\n",
    "        ):\n",
    "            output_logged += 1\n",
    "        \n",
    "        # Log trajectory evaluation result\n",
    "        traj_score = trajectory_report.scores[i] if i < len(trajectory_report.scores) else 0\n",
    "        traj_reason = trajectory_report.reasons[i] if i < len(trajectory_report.reasons) else \"\"\n",
    "        if send_evaluation_to_cloudwatch(\n",
    "            trace_id=trace_id,\n",
    "            session_id=SESSION_ID,\n",
    "            evaluator_name=TRAJECTORY_EVALUATOR_NAME,\n",
    "            score=traj_score,\n",
    "            explanation=str(traj_reason)[:500],\n",
    "            config_id=EVALUATION_CONFIG_ID,\n",
    "        ):\n",
    "            trajectory_logged += 1\n",
    "    \n",
    "    print(f\"CloudWatch logging complete:\")\n",
    "    print(f\"  Output evaluations logged: {output_logged}/{len(cases)}\")\n",
    "    print(f\"  Trajectory evaluations logged: {trajectory_logged}/{len(cases)}\")\n",
    "else:\n",
    "    if not LOG_TO_CLOUDWATCH:\n",
    "        print(\"CloudWatch logging disabled. Set LOG_TO_CLOUDWATCH = True to enable.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Aggregate results showing how well the agent matched ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cases:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"SUMMARY\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\nSession: {SESSION_ID}\")\n",
    "    print(f\"Mode: {'Demo' if USE_DEMO_MODE else 'Live'}\")\n",
    "    print(f\"Traces Evaluated: {len(cases)}\")\n",
    "    \n",
    "    print(f\"\\nOutput Evaluation (Actual vs Expected Response):\")\n",
    "    print(f\"  Overall Score: {output_report.overall_score:.2f}\")\n",
    "    print(f\"  Range: {min(output_report.scores):.2f} - {max(output_report.scores):.2f}\")\n",
    "    \n",
    "    print(f\"\\nTrajectory Evaluation (Actual vs Expected Tools):\")\n",
    "    print(f\"  Overall Score: {trajectory_report.overall_score:.2f}\")\n",
    "    print(f\"  Range: {min(trajectory_report.scores):.2f} - {max(trajectory_report.scores):.2f}\")\n",
    "    \n",
    "    low_output = [i+1 for i, s in enumerate(output_report.scores) if s < 0.5]\n",
    "    low_traj = [i+1 for i, s in enumerate(trajectory_report.scores) if s < 0.5]\n",
    "    \n",
    "    if low_output:\n",
    "        print(f\"\\nTraces with low output scores (<0.5): {low_output}\")\n",
    "    if low_traj:\n",
    "        print(f\"Traces with low trajectory scores (<0.5): {low_traj}\")\n",
    "    \n",
    "    if not low_output and not low_traj:\n",
    "        print(f\"\\nAll traces scored above 0.5 - agent behavior matches ground truth well!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Results\n",
    "\n",
    "Save evaluation results to JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cases:\n",
    "    export_data = {\n",
    "        \"evaluation_time\": datetime.now(timezone.utc).isoformat(),\n",
    "        \"session_id\": SESSION_ID,\n",
    "        \"mode\": \"demo\" if USE_DEMO_MODE else \"live\",\n",
    "        \"evaluation_type\": \"ground_truth\",\n",
    "        \"summary\": {\n",
    "            \"traces_evaluated\": len(cases),\n",
    "            \"output_overall_score\": output_report.overall_score,\n",
    "            \"trajectory_overall_score\": trajectory_report.overall_score,\n",
    "        },\n",
    "        \"traces\": [\n",
    "            {\n",
    "                \"trace_id\": case.metadata.get(\"trace_id\"),\n",
    "                \"user_prompt\": case.input,\n",
    "                \"expected_output\": case.expected_output,\n",
    "                \"actual_output\": case.metadata.get(\"actual_output\"),\n",
    "                \"expected_trajectory\": case.metadata.get(\"expected_trajectory\"),\n",
    "                \"actual_trajectory\": case.metadata.get(\"actual_trajectory\"),\n",
    "                \"output_score\": output_report.scores[i] if i < len(output_report.scores) else None,\n",
    "                \"trajectory_score\": trajectory_report.scores[i] if i < len(trajectory_report.scores) else None,\n",
    "            }\n",
    "            for i, case in enumerate(cases)\n",
    "        ],\n",
    "    }\n",
    "    \n",
    "    output_path = \"ground_truth_results.json\"\n",
    "    with open(output_path, \"w\") as f:\n",
    "        json.dump(export_data, f, indent=2)\n",
    "    \n",
    "    print(f\"Results exported to {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
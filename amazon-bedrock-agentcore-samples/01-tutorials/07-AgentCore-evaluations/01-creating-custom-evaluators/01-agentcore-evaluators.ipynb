{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "## AgentCore Evaluations - creating evaluators\n",
    "\n",
    "In this tutorial you will learn about AgentCore Evaluations built-in and custom metrics.\n",
    "You'll learn when to use each type and how to create custom evaluators tailored to your specific needs.\n",
    "\n",
    "### What You'll Learn\n",
    "- Understanding built-in evaluators and their use cases\n",
    "- Creating custom evaluators for specialized requirements\n",
    "- Selecting the right evaluation approach for your agents\n",
    "\n",
    "### Tutorial Details\n",
    "\n",
    "| Information         | Details                                                                        |\n",
    "|:--------------------|:-------------------------------------------------------------------------------|\n",
    "| Tutorial type       | Create custom evaluatior metric                                                |\n",
    "| LLM model           | Anthropic Claude Haiku 4.5                                                     |\n",
    "| Tutorial components | Listing built-in evaluators, creating a custom evaluator metric                |\n",
    "| Tutorial vertical   | Cross-vertical                                                                 |\n",
    "| Example complexity  | Easy                                                                           |\n",
    "| SDK used            | Amazon Bedrock AgentCore Starter toolkit                                       |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ecec088-9de0-4637-b35b-9c918ffb793e",
   "metadata": {},
   "source": [
    "## Evaluators Types\n",
    "\n",
    "### Built-in evaluators\n",
    "\n",
    "AgentCore provides 13 pre-configured evaluators that use Large Language Models (LLMs) as judges to assess agent performance. These evaluators come with their carefully crafted prompt templates and pre-selected evaluator models, standardizing the evaluation criteria across different use cases. They are ready to use and you don't need to add any additional configuration to get started.\n",
    "\n",
    "These evaluators are divided into 4 different groups:\n",
    "\n",
    "- **Response quality**: evaluators that help you deciding if your agent is working as expected at each turn. They work on a trace-basis and evaluate each user-agent interaction.\n",
    "- **Task completion**: this category has one evaluator (Goal success) that will evaluate the session as a whole. In a multi-turn conversation, this evaluator helps you deciding if the user goal (or goals) were completed and if the final outcome was achieved. In situations where the agent asks follow up questions, this evaluator is essential to understand if the tasks requested were actually completed.\n",
    "- **Tool level**: evaluators that help you understand how sucessful your tool calling is. It measures the accuracy of the tool and parameter selection for the agent at a tool level. If an agent is calling two or more different tools in a single turn, each one of them will have its own metric in the trace.\n",
    "- **Safety**: evaluators that detect if your agent is being harmful or is making steriotyping generalizations about individuals or groups.\n",
    "\n",
    "When using built-in metrics everything is handled for you from prompt to model. That means that your evaluator cannot be modified, in order to maintain the evaluation consistent and reliable across all users. You can however create your own metrics using a built-in metric as basis. To do so, we provide you with the **Prompt Templates** for our built-in evaluators.\n",
    "\n",
    "### Custom evaluators\n",
    "\n",
    "Custom evaluators provide maximum flexibility by allowing you to define every aspect of your evaluation process while leveraging LLMs as underlying judges. You can customize the following in your custom evaluator:\n",
    "\n",
    "- **Evaluator model**: Choose the LLM that best fits your evaluation needs\n",
    "- **Evaluation prompts**: Craft evaluation instructions specific to your use case\n",
    "- **Scoring schema**: Design scoring systems that align with your organization's metrics\n",
    "\n",
    "### Generating traces on AgentCore Observability from an agent\n",
    "\n",
    "AgentCore Observability provides comprehensive visibility into agent behavior during invocations by leveraging [OpenTelemetry (OTEL)](https://opentelemetry.io/) traces as the foundation for capturing and structuring detailed execution data. AgentCore relies on [AWS Distro for OpenTelemetry (ADOT)](https://aws-otel.github.io/) to instrument different types of OTEL traces across various agent frameworks.\n",
    "\n",
    "When your agent is hosted on AgentCore Runtime (like our agent in this tutorial), the AgentCore Observability instrumentation is automatic, with minimal configuration. All you need to do is include `aws-opentelemetry-distro` in `requirements.txt` and AgentCore Runtime handles OTEL configuration automatically. When your agent is not running in AgentCore Runtime, you will need to instrument it with ADOT to have it available in AgentCore Observability. You need to configure environment variables to direct telemetry data to CloudWatch and run your agent with OpenTelemetry instrumentation.\n",
    "\n",
    "The process looks as following:\n",
    "\n",
    "![session_traces](../images/observability_traces.png)\n",
    "\n",
    "Once your session traces are available in AgentCore Observability, you can use AgentCore Evaluations to evaluate your agent's behavior.\n",
    "\n",
    "### Evaluation levels\n",
    "AgentCore Evaluations operate in different levels of the agent interaction. You can analyse the back and forward conversation as a whole using the session information. You can also evaluate the agent's response to a user question in an individual turn of the conversation using the trace information. Or you can evaluate information inside of a turn, which includes the tool calling and parameter selection, using the span data.\n",
    "\n",
    "You can create custom metrics for the different levels. The built-in metrics operate in the following scope:\n",
    "\n",
    "![metrics level](../images/metrics_per_level.png)\n",
    "\n",
    "In this tutorial, we will create a metric at trace level.\n",
    "\n",
    "### Tutorial outcomes\n",
    "\n",
    "By the end of this tutorial you will have learned about the AgentCore Evaluation built-in and custom metrics. You will also have created a custom metric to measure the response quality of your agents. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8746ae9b-2362-4dd7-884d-b82b68b44c32",
   "metadata": {},
   "source": [
    "### Prerequisites\n",
    "To execute this tutorial you will need:\n",
    "* Python 3.10+\n",
    "* AWS credentials\n",
    "* Amazon Bedrock AgentCore Starter toolkit\n",
    "\n",
    "### Using AgentCore Evaluations\n",
    "\n",
    "Amazon Bedrock AgentCore supports various interfaces for developing, deploying and monitoring your agents and tools.\n",
    "\n",
    "For full control, you can use the [control plane](https://docs.aws.amazon.com/bedrock-agentcore-control/latest/APIReference/Welcome.html) and [data plane](https://docs.aws.amazon.com/bedrock-agentcore/latest/APIReference/Welcome.html) APIs. Those APIs are exposed via AWS SDKs ([boto3](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/quickstart.html), [AWS SDK for Java](https://docs.aws.amazon.com/sdk-for-java/latest/developer-guide/home.html), [AWS SDK for JavaScript](https://docs.aws.amazon.com/sdk-for-javascript/v3/developer-guide/welcome.html)) and other AWS developer tools, such as the AWS Command Line Interface ([AWS CLI](https://aws.amazon.com/cli/)).\n",
    "\n",
    "For a simplified approach, you can also use the [AgentCore Python SDK](https://github.com/aws/bedrock-agentcore-sdk-python) and the [AgentCore Starter Toolkit](https://github.com/aws/bedrock-agentcore-starter-toolkit). The AgentCore Python SDK provides Python primitives for agent development and the AgentCore Starter Toolkit provides CLI tools and higher-level abstractions for AgentCore functionalities.\n",
    "\n",
    "![agentcore_interfaces](../images/agentcore_interfaces.png)\n",
    "\n",
    "For this tutorial, we will use the AgentCore Starter Toolkit for a simplified experience. In the `03-advanced` folder you can find an example of working with boto3 directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6af26de-7357-42e5-b9df-05b0cd3191c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bedrock_agentcore_starter_toolkit import Evaluation\n",
    "import os\n",
    "import json\n",
    "from boto3.session import Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4678b509-d9a1-4a5f-bf24-4bb4bf7cd2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "boto_session = Session()\n",
    "region = boto_session.region_name\n",
    "print(region)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc91f9c2-b206-4086-9d8f-4324780d5820",
   "metadata": {},
   "source": [
    "### Initiating the AgentCore evaluation client\n",
    "\n",
    "Let's now initiate our evaluation client. For this tutorial we will use the [AgentCore Starter Toolkit](https://github.com/aws/bedrock-agentcore-starter-toolkit), an abstraction SDK that simplifies your interaction with the AgentCore components to speed up your getting started process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f47543-189e-4cae-aad8-b82c55048dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_client = Evaluation(region=region)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df4654f-7a66-47c8-975d-9427b9d6ed22",
   "metadata": {},
   "source": [
    "### Retrieving built-in evaluators\n",
    "\n",
    "Let's now retrieve the available built-in evaluators to understand where they can be used. The `list_evaluators()` function can help with it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413407d0-3e7c-40fe-abfe-3cfc08de4bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "available_evaluators = eval_client.list_evaluators()\n",
    "available_evaluators"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d66a84-3b38-410a-b940-6608f13f2e59",
   "metadata": {},
   "source": [
    "We can also retrieve the information as a dictionary to use in on-demand and online evaluations later on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7424b670-7a1b-4360-8402-ca818bccf833",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    available_evaluators['evaluators'][0]['evaluatorId'], \n",
    "    available_evaluators['evaluators'][0]['description']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b26425-59ac-4d30-80aa-eee31eaaf2cd",
   "metadata": {},
   "source": [
    "As we can see, the `Builtin.Correctness` metric help us evaluate a response quality. Let's deep-dive into this metric to undersand its details. For that we can use the `get_evaluator` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104cc125-e0eb-4a70-ab98-6089edc8815f",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_client.get_evaluator(evaluator_id=\"Builtin.Correctness\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f36da8-59a0-4ab7-aa31-a9d48e200aee",
   "metadata": {},
   "source": [
    "In this case we can see that our evaluator is classifying the response into 3 levels: Incorrect, Partially Correct and Correct. For our use case, we would like to be a bit more detailed and use a 5 levels scale. To help with that, we can create a custom evaluator.\n",
    "\n",
    "### Create custom evaluator\n",
    "\n",
    "Let's now create a custom metric for response quality that will allow us to have the 5 levels scale. To do so we need to select an evaluator model, provide instructions to the evaluation and set the rating scale. In this case our scale will go from Very Good to Very Poor. Let's retrieve the evaluation configuration from the JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df380f04-e28a-446f-b510-b5b82b6cc66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"metric.json\") as f:\n",
    "    print(\"Reading custom metric details\")\n",
    "    eval_config = json.load(f)\n",
    "eval_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c178aa-6ab2-4568-9848-7fe65c291358",
   "metadata": {},
   "source": [
    "We can then use the `create_evaluator` method to create the evaluator. We will create the `response_Quality` evaluator at `TRACE` level. \n",
    "\n",
    "When creating your custom evaluator you can chose to apply it at a tool call level, trace level or session level. \n",
    "\n",
    "* **A tool call** is a span that represents an agentâ€™s invocation of an external function, API, or capability. Tool-call spans typically capture information such as the tool name, input parameters, execution time, and output. Tool-call details are used to evaluate whether the agent selected and used tools correctly and efficiently.\n",
    "\n",
    "* **A trace** is a complete record of a single agent execution or request. A trace contains one or more spans, which represent the individual operations performed during that execution. Traces provide end-to-end visibility into agent decisions and tool usage.\n",
    "\n",
    "* **A session** represents a logical grouping of related interactions from a single user or workflow. A session may contain one or more traces. Sessions help you view and evaluate agent behavior across multi-step interactions, rather than focusing on individual requests. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bc41fb-c552-4843-8520-c9ffa28d860a",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_evaluator = eval_client.create_evaluator(\n",
    "    name=\"response_quality_for_scope\",\n",
    "    level=\"TRACE\",\n",
    "    description=\"Response quality evaluator\",\n",
    "    config=eval_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c93ce0f-8088-46a1-9a8c-756f1eeaf3df",
   "metadata": {},
   "source": [
    "### Saving evaluator information for next tutorials\n",
    "\n",
    "We will now save the evaluatorId for usage in the next tutorials. We will save the `evaluator_id` variable for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521cd705-5849-46fd-8a3d-082a76fb8e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator_id = custom_evaluator['evaluatorId']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3a7126-f1c6-4896-9c9d-185257b90da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store evaluator_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d69d843-0a86-4172-a775-dbaa9888f5dd",
   "metadata": {},
   "source": [
    "#### Congratulations\n",
    "\n",
    "You have now created a custom evaluator that we will use in the next tutorials."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

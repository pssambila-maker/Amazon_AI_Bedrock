{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Analyzer\n",
    "\n",
    "Scale your AI agent evaluation analysis from days/weeks to minutes.\n",
    "\n",
    "**The Problem:** At scale, LLM-as-a-Judge evaluations produce hundreds of score/explanation pairs. Reading through all of them to find patterns is time-consuming, error-prone, and doesn't scale.\n",
    "\n",
    "**The Solution:** This notebook uses AI to analyze AI evaluations results, identifies systematic problems using the evaluation explanations, and generates specific system prompt fixes for the AI Agent being evaluated. The developer is responsible to use the suggested prompts and updated the AI Agent manually/automatically to test our the improvments driven with the proposed fix.\n",
    "\n",
    "## Where This Fits\n",
    "\n",
    "AI Agent evaluation is a continuous loop where you test the agent using your test cases, analyze the responses via GroundTruth or via context-based evaluations using an LLM, analyze the evaluation results, make improvements to your agent, and repeat unless your success criteria is met to launch the agent to production.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"assets/improvement_loop.svg\" alt=\"Continuous improvement loop\" width=\"700\">\n",
    "</p>\n",
    "\n",
    "## How It Works\n",
    "\n",
    "Built with [Strands Agents SDK](https://github.com/strands-agents/sdk-python):\n",
    "\n",
    "- **Orchestrator** receives all low-scoring evaluations and your system prompt\n",
    "- Calls `analyze_batch()` tool for each batch → invokes **Batch Analyzer** sub-agent\n",
    "- **Batch Analyzer** reads LLM judge explanations, groups failures into patterns, extracts evidence quotes\n",
    "- Orchestrator aggregates patterns across batches, ranks by frequency × severity\n",
    "- Generates final report with top 3 problems and copy-paste-ready prompt fixes\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"assets/architecture.svg\" alt=\"Architecture diagram\" width=\"700\">\n",
    "</p>\n",
    "\n",
    "## Input Sources\n",
    "\n",
    "- [Strands Evaluation Output](https://github.com/strands-agents/strands-evals)\n",
    "- [AWS AgentCore Evaluation  Output](https://docs.aws.amazon.com/agentcore/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "EVAL_FOLDER = \"eval_data/\"\n",
    "BATCH_SIZE = 10\n",
    "SCORE_THRESHOLD = 0.7\n",
    "SYSTEM_PROMPT_FILE = \"system_prompt.txt\"\n",
    "MODEL_ID = \"us.anthropic.claude-sonnet-4-5-20250929-v1:0\"\n",
    "AWS_REGION = \"us-east-1\"\n",
    "\n",
    "# Load system prompt\n",
    "from pathlib import Path\n",
    "\n",
    "prompt_path = Path(SYSTEM_PROMPT_FILE)\n",
    "if not prompt_path.exists() or prompt_path.stat().st_size < 100:\n",
    "    print(f\"Warning: {SYSTEM_PROMPT_FILE} not found, using example_system_prompt.txt\")\n",
    "    SYSTEM_PROMPT_FILE = \"example_system_prompt.txt\"\n",
    "\n",
    "with open(SYSTEM_PROMPT_FILE, 'r') as f:\n",
    "    AGENT_SYSTEM_PROMPT = f.read()\n",
    "\n",
    "# Strip comment header if present\n",
    "lines = AGENT_SYSTEM_PROMPT.split('\\n')\n",
    "content_lines = []\n",
    "in_header = True\n",
    "for line in lines:\n",
    "    if in_header and line.startswith('#'):\n",
    "        continue\n",
    "    in_header = False\n",
    "    content_lines.append(line)\n",
    "AGENT_SYSTEM_PROMPT = '\\n'.join(content_lines).strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Optional\n",
    "from statistics import mean, stdev\n",
    "from collections import defaultdict\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "from strands import Agent, tool\n",
    "from strands.models import BedrockModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_evaluations(data: Any, parent_metadata: Optional[Dict] = None) -> List[Dict]:\n",
    "    \"\"\"Recursively extract evaluations from any JSON structure.\"\"\"\n",
    "    evaluations = []\n",
    "    parent_metadata = parent_metadata or {}\n",
    "    \n",
    "    if isinstance(data, list):\n",
    "        for item in data:\n",
    "            evaluations.extend(extract_evaluations(item, parent_metadata))\n",
    "    elif isinstance(data, dict):\n",
    "        score_key = 'score' if 'score' in data else ('value' if 'value' in data else None)\n",
    "        \n",
    "        if score_key and 'explanation' in data:\n",
    "            eval_entry = {\n",
    "                'score': data[score_key],\n",
    "                'explanation': data['explanation'],\n",
    "                'metadata': {**parent_metadata}\n",
    "            }\n",
    "            for key, val in data.items():\n",
    "                if key not in ['score', 'value', 'explanation']:\n",
    "                    if isinstance(val, (str, int, float, bool)):\n",
    "                        eval_entry['metadata'][key] = val\n",
    "            evaluations.append(eval_entry)\n",
    "        else:\n",
    "            nested_metadata = {**parent_metadata}\n",
    "            for key in ['session_id', 'trace_id', 'evaluator_name', 'evaluator_id']:\n",
    "                if key in data:\n",
    "                    nested_metadata[key] = data[key]\n",
    "            for key in ['results', 'evaluations', 'data', 'items']:\n",
    "                if key in data:\n",
    "                    evaluations.extend(extract_evaluations(data[key], nested_metadata))\n",
    "    \n",
    "    return evaluations\n",
    "\n",
    "def load_evaluations(folder_path: str) -> List[Dict]:\n",
    "    \"\"\"Load all JSON files from a folder and extract evaluations.\"\"\"\n",
    "    evaluations = []\n",
    "    folder = Path(folder_path)\n",
    "    \n",
    "    for json_file in sorted(folder.glob(\"*.json\")):\n",
    "        try:\n",
    "            with open(json_file, 'r') as f:\n",
    "                data = json.load(f)\n",
    "            evals = extract_evaluations(data, {'source_file': json_file.name})\n",
    "            evaluations.extend(evals)\n",
    "            print(f\"  {json_file.name}: {len(evals)} evaluations\")\n",
    "        except Exception as e:\n",
    "            print(f\"  {json_file.name}: Error - {e}\")\n",
    "    \n",
    "    return evaluations\n",
    "\n",
    "def compute_statistics(evaluations: List[Dict], threshold: float) -> Dict:\n",
    "    \"\"\"Compute statistics from evaluations.\"\"\"\n",
    "    if not evaluations:\n",
    "        return {'total': 0, 'error': 'No evaluations found'}\n",
    "    \n",
    "    scores = [e['score'] for e in evaluations if e['score'] is not None]\n",
    "    \n",
    "    by_evaluator = defaultdict(list)\n",
    "    for e in evaluations:\n",
    "        evaluator = e['metadata'].get('evaluator_name', e['metadata'].get('label', 'unknown'))\n",
    "        if e['score'] is not None:\n",
    "            by_evaluator[evaluator].append(e['score'])\n",
    "    \n",
    "    evaluator_stats = {}\n",
    "    for evaluator, eval_scores in by_evaluator.items():\n",
    "        evaluator_stats[evaluator] = {\n",
    "            'count': len(eval_scores),\n",
    "            'mean': round(mean(eval_scores), 3),\n",
    "            'min': round(min(eval_scores), 3),\n",
    "            'max': round(max(eval_scores), 3),\n",
    "        }\n",
    "        if len(eval_scores) > 1:\n",
    "            evaluator_stats[evaluator]['stdev'] = round(stdev(eval_scores), 3)\n",
    "    \n",
    "    low_scoring = [e for e in evaluations if e['score'] is not None and e['score'] < threshold]\n",
    "    \n",
    "    return {\n",
    "        'total': len(evaluations),\n",
    "        'valid_scores': len(scores),\n",
    "        'mean_score': round(mean(scores), 3) if scores else None,\n",
    "        'min_score': round(min(scores), 3) if scores else None,\n",
    "        'max_score': round(max(scores), 3) if scores else None,\n",
    "        'stdev': round(stdev(scores), 3) if len(scores) > 1 else None,\n",
    "        'low_scoring_count': len(low_scoring),\n",
    "        'low_scoring_pct': round(len(low_scoring) / len(scores) * 100, 1) if scores else 0,\n",
    "        'by_evaluator': evaluator_stats,\n",
    "        'threshold': threshold\n",
    "    }\n",
    "\n",
    "def batch_evaluations(evaluations: List[Dict], batch_size: int) -> List[List[Dict]]:\n",
    "    \"\"\"Split evaluations into batches.\"\"\"\n",
    "    return [evaluations[i:i+batch_size] for i in range(0, len(evaluations), batch_size)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Load and Validate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate configuration\n",
    "if not AGENT_SYSTEM_PROMPT.strip():\n",
    "    raise ValueError(f\"System prompt is empty. Edit {SYSTEM_PROMPT_FILE}\")\n",
    "\n",
    "if \"example_system_prompt\" in SYSTEM_PROMPT_FILE:\n",
    "    print(f\"Using example system prompt. Edit system_prompt.txt for best results.\")\n",
    "else:\n",
    "    print(f\"System prompt loaded ({len(AGENT_SYSTEM_PROMPT)} chars)\")\n",
    "\n",
    "eval_path = Path(EVAL_FOLDER)\n",
    "if not eval_path.exists():\n",
    "    raise ValueError(f\"Evaluation folder not found: {EVAL_FOLDER}\")\n",
    "\n",
    "json_files = list(eval_path.glob(\"*.json\"))\n",
    "if not json_files:\n",
    "    raise ValueError(f\"No JSON files found in {EVAL_FOLDER}\")\n",
    "print(f\"Found {len(json_files)} JSON files in {EVAL_FOLDER}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Loading evaluations from {EVAL_FOLDER}:\")\n",
    "evaluations = load_evaluations(EVAL_FOLDER)\n",
    "print(f\"\\nTotal: {len(evaluations)} evaluations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = compute_statistics(evaluations, SCORE_THRESHOLD)\n",
    "\n",
    "print(f\"Mean score: {stats['mean_score']} (range: {stats['min_score']} - {stats['max_score']})\")\n",
    "print(f\"Low scoring (<{SCORE_THRESHOLD}): {stats['low_scoring_count']} ({stats['low_scoring_pct']}%)\")\n",
    "\n",
    "print(\"\\nBy evaluator:\")\n",
    "for evaluator, eval_stats in stats['by_evaluator'].items():\n",
    "    print(f\"  {evaluator}: mean={eval_stats['mean']}, count={eval_stats['count']}\")\n",
    "\n",
    "low_scoring = [e for e in evaluations if e['score'] is not None and e['score'] < SCORE_THRESHOLD]\n",
    "print(f\"\\n{len(low_scoring)} low-scoring evaluations will be analyzed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Define Analysis Agents\n",
    "\n",
    "This step creates two agents using [Strands Agents SDK](https://github.com/strands-agents/sdk-python):\n",
    "\n",
    "**Orchestrator** (main agent)<br>\n",
    "**Batch Analyzer** (sub-agent)\n",
    "\n",
    "**Flow:**  `Orchestrator` → `analyze_batch()` tool → `Batch Analyzer` → JSON patterns → Orchestrator synthesizes → `Report` -> `User updates System Prompt of their AI Agent under evaluation`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# AGENT PROMPTS\n",
    "# ============================================\n",
    "\n",
    "BATCH_ANALYZER_PROMPT = \"\"\"\n",
    "You analyze low-scoring evaluations to identify systematic failure patterns.\n",
    "\n",
    "## Input\n",
    "A batch of evaluations, each with:\n",
    "- score: numeric 0-1 (scores < 0.7 indicate problems)\n",
    "- explanation: detailed text from the LLM judge explaining why the score was given\n",
    "- metadata: context including evaluator_name, trace_id/session_id\n",
    "\n",
    "## Your Task\n",
    "1. Read each explanation carefully - these contain the LLM judge's reasoning\n",
    "2. Identify SYSTEMATIC PATTERNS (not isolated incidents) in why scores are low\n",
    "3. Group similar failures together\n",
    "4. Extract 2-3 specific quotes as evidence per pattern\n",
    "5. Note which evaluator metrics are affected\n",
    "\n",
    "## Output (JSON)\n",
    "Return ONLY valid JSON with this structure:\n",
    "{\n",
    "  \"patterns\": [\n",
    "    {\n",
    "      \"name\": \"short descriptive name\",\n",
    "      \"description\": \"what the agent did wrong and why it's problematic\",\n",
    "      \"count\": N,\n",
    "      \"evaluators_affected\": [\"Faithfulness\", \"Correctness\"],\n",
    "      \"evidence\": [\"direct quote from explanation 1\", \"direct quote from explanation 2\"],\n",
    "      \"root_cause\": \"what's missing or unclear in the system prompt\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\n",
    "CONSTRAINTS:\n",
    "- Maximum 5 patterns per batch\n",
    "- Only include patterns that appear 2+ times (systematic, not isolated)\n",
    "- Evidence quotes should be verbatim from explanations\n",
    "- Root cause should identify what prompt guidance would fix this\n",
    "\"\"\"\n",
    "\n",
    "ORCHESTRATOR_PROMPT = \"\"\"\n",
    "You synthesize evaluation analysis into actionable recommendations with system prompt improvements.\n",
    "\n",
    "## Your Task\n",
    "1. Use the analyze_batch tool to analyze low-scoring evaluations in batches\n",
    "2. Collect patterns from all batches\n",
    "3. Identify the TOP 3 most impactful problems based on:\n",
    "   - **Frequency**: Appears across multiple evaluations (the more, the worse)\n",
    "   - **Severity**: Lower scores indicate more severe problems\n",
    "   - **Fixability**: Can be addressed by clarifying the system prompt\n",
    "4. Generate specific, minimal prompt changes to fix each problem\n",
    "\n",
    "## Required Output Format\n",
    "\n",
    "# Evaluation Analysis Report\n",
    "\n",
    "## Summary\n",
    "[2-3 sentences on overall health of the agent based on the statistics and patterns found]\n",
    "\n",
    "## Top 3 Problems\n",
    "\n",
    "### Problem 1: [Specific Descriptive Name]\n",
    "\n",
    "**Evidence from evaluations:**\n",
    "- \"[Direct quote from LLM judge explanation]\"\n",
    "- \"[Another direct quote showing this pattern]\"\n",
    "\n",
    "**Frequency & Impact:**\n",
    "- Appears in X out of Y low-scoring evaluations\n",
    "- Affects metrics: [list evaluator names]\n",
    "- Average score when this occurs: X.XX\n",
    "\n",
    "**Root Cause:**\n",
    "[What's missing or unclear in the current system prompt that causes this behavior]\n",
    "\n",
    "**Proposed Fix:**\n",
    "[Specific text to add/modify in the prompt and why it will work]\n",
    "\n",
    "---\n",
    "\n",
    "### Problem 2: [Specific Descriptive Name]\n",
    "\n",
    "**Evidence from evaluations:**\n",
    "- \"[Direct quote]\"\n",
    "- \"[Another quote]\"\n",
    "- \"[TraceID and SessionID]\"\n",
    "\n",
    "**Frequency & Impact:**\n",
    "- Appears in X out of Y low-scoring evaluations\n",
    "- Affects metrics: [list]\n",
    "- Average score when this occurs: X.XX\n",
    "\n",
    "**Root Cause:**\n",
    "[What's missing in the prompt]\n",
    "\n",
    "**Proposed Fix:**\n",
    "[Specific change and rationale]\n",
    "\n",
    "---\n",
    "\n",
    "### Problem 3: [Specific Descriptive Name]\n",
    "\n",
    "**Evidence from evaluations:**\n",
    "- \"[Direct quote]\"\n",
    "- \"[Another quote]\"\n",
    "\n",
    "**Frequency & Impact:**\n",
    "- Appears in X out of Y low-scoring evaluations\n",
    "- Affects metrics: [list]\n",
    "- Average score when this occurs: X.XX\n",
    "\n",
    "**Root Cause:**\n",
    "[What's missing in the prompt]\n",
    "\n",
    "**Proposed Fix:**\n",
    "[Specific change and rationale]\n",
    "\n",
    "---\n",
    "\n",
    "## Suggested System Prompt Changes\n",
    "\n",
    "### Changes Summary\n",
    "| # | What Changed | Original Text | New Text | Fixes |\n",
    "|---|--------------|---------------|----------|-------|\n",
    "| 1 | [brief description] | [exact original snippet] | [exact new snippet] | Problem 1 |\n",
    "| 2 | [brief description] | [exact original snippet] | [exact new snippet] | Problem 2 |\n",
    "| 3 | [brief description] | [exact original snippet] | [exact new snippet] | Problem 3 |\n",
    "\n",
    "### Complete Updated System Prompt\n",
    "```\n",
    "[FULL UPDATED PROMPT - COPY-PASTE READY]\n",
    "```\n",
    "\n",
    "## CONSTRAINTS\n",
    "- Only 3 problems, ranked by impact (frequency × severity)\n",
    "- Evidence must be actual quotes from the evaluation explanations with traceID and sessionID\n",
    "- Make minimal, surgical prompt changes (not a complete rewrite)\n",
    "- Preserve everything in the original prompt that works well\n",
    "- The Complete Updated System Prompt must be the FULL prompt, ready to use\n",
    "- No implementation roadmaps, KPIs, timelines, or risk assessments\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BedrockModel(model_id=MODEL_ID, region_name=AWS_REGION)\n",
    "\n",
    "batch_analyzer_agent = Agent(model=model, system_prompt=BATCH_ANALYZER_PROMPT)\n",
    "\n",
    "@tool\n",
    "def analyze_batch(batch_json: str) -> str:\n",
    "    \"\"\"Analyze a batch of low-scoring evaluations to identify failure patterns.\"\"\"\n",
    "    result = batch_analyzer_agent(f\"Analyze these evaluations and return JSON patterns:\\n{batch_json}\")\n",
    "    return str(result)\n",
    "\n",
    "orchestrator = Agent(model=model, system_prompt=ORCHESTRATOR_PROMPT, tools=[analyze_batch])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 5: Run the Analysis\n",
    "\n",
    "The orchestrator will:\n",
    "1. Process evaluations in batches using the sub-agent\n",
    "2. Identify the top 3 problems\n",
    "3. Generate specific prompt improvements\n",
    "4. Output a complete updated system prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Analyzing {len(low_scoring)} evaluations in {len(batch_evaluations(low_scoring, BATCH_SIZE))} batches...\")\n",
    "\n",
    "batches = batch_evaluations(low_scoring, BATCH_SIZE)\n",
    "batches_json = [json.dumps(batch, indent=2) for batch in batches]\n",
    "\n",
    "analysis_prompt = f\"\"\"\n",
    "Analyze these evaluation results and provide a comprehensive report.\n",
    "\n",
    "## Statistics\n",
    "- Total evaluations: {stats['total']}\n",
    "- Mean score: {stats['mean_score']}\n",
    "- Low scoring (<{SCORE_THRESHOLD}): {stats['low_scoring_count']} ({stats['low_scoring_pct']}%)\n",
    "- Score range: {stats['min_score']} - {stats['max_score']}\n",
    "\n",
    "## Evaluator Breakdown\n",
    "{json.dumps(stats['by_evaluator'], indent=2)}\n",
    "\n",
    "## Current Agent System Prompt (to be improved)\n",
    "{AGENT_SYSTEM_PROMPT}\n",
    "\n",
    "## Low-Scoring Evaluations\n",
    "There are {len(batches)} batches of evaluations to analyze.\n",
    "Use the analyze_batch tool for each batch:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "for i, batch_json in enumerate(batches_json):\n",
    "    analysis_prompt += f\"\\nBatch {i+1}:\\n{batch_json}\\n\"\n",
    "\n",
    "start_time = time.time()\n",
    "result = orchestrator(analysis_prompt)\n",
    "elapsed_time = round(time.time() - start_time, 2)\n",
    "result_text = str(result)\n",
    "\n",
    "print(f\"Analysis complete ({elapsed_time}s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: View Results\n",
    "\n",
    "After running this notebook:\n",
    "\n",
    "- **Top 3 problems** with evidence quotes from the LLM judge\n",
    "- **Root cause analysis** for each problem\n",
    "- **Before/after changes table** showing exact prompt modifications\n",
    "- **Complete updated system prompt** ready to copy-paste\n",
    "\n",
    "See [example_agent_output.md](example_agent_output.md) for a sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# DISPLAY RESULTS\n",
    "# ============================================\n",
    "\n",
    "display(Markdown(result_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_file = f\"analysis_report_{timestamp}.md\"\n",
    "\n",
    "report_content = f\"\"\"# Evaluation Analysis Report\n",
    "\n",
    "**Generated:** {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}\n",
    "**Evaluations Analyzed:** {len(low_scoring)} low-scoring out of {stats['total']} total\n",
    "**Processing Time:** {elapsed_time}s\n",
    "\n",
    "---\n",
    "\n",
    "{result_text}\n",
    "\"\"\"\n",
    "\n",
    "with open(output_file, \"w\") as f:\n",
    "    f.write(report_content)\n",
    "\n",
    "print(f\"Report saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "After running this analysis:\n",
    "\n",
    "1. **Review the report** - Check if the identified problems match your observations\n",
    "2. **Copy the updated prompt** - Use the \"Complete Updated System Prompt\" section\n",
    "3. **Test incrementally** - Apply one change at a time if preferred\n",
    "4. **Re-evaluate** - Run your evaluation suite again to measure improvement\n",
    "5. **Iterate** - Repeat this process until scores meet your targets\n",
    "\n",
    "### Tips\n",
    "\n",
    "- If you have many low-scoring evaluations, increase `BATCH_SIZE` to reduce API calls\n",
    "- Focus on the highest-impact problems first (those affecting multiple evaluators)\n",
    "- Keep your original system prompt backed up before making changes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0df90111-38c8-4032-889a-0ee5a10f6132",
   "metadata": {},
   "source": [
    "## AgentCore Evaluations - on-demand evaluation for Strands Agent\n",
    "\n",
    "In this tutorial you will learn about to use the on-demand evaluation from AgentCore Evaluations applied to a Strands agent.\n",
    "\n",
    "To execute this lab you should first have created the Strands agent using the code at [00-prereqs](../../00-prereqs) folder and created your custom evaluator using the code at [01-creating-custom-evaluators](../../01-creating-custom-evaluators)\n",
    "\n",
    "### What You'll Learn\n",
    "- How to run on-demand evaluations to a trace using the AgentCore Starter toolkit\n",
    "\n",
    "### Tutorial Details\n",
    "\n",
    "| Information         | Details                                                                       |\n",
    "|:--------------------|:------------------------------------------------------------------------------|\n",
    "| Tutorial type       | Evaluating Strands agent with on-demand evaluators (built-in and custom)      |\n",
    "| Tutorial components | Running evaluation with built-in and custom evaluators                        |\n",
    "| Tutorial vertical   | Cross-vertical                                                                |\n",
    "| Example complexity  | Easy                                                                          |\n",
    "| SDK used            | Amazon Bedrock AgentCore Starter toolkit                                      |\n",
    "\n",
    "### On-demand evaluation\n",
    "\n",
    "On-demand evaluation provides a flexible way to evaluate specific agent interactions by directly analyzing a chosen set of spans. Unlike online evaluation which continuously monitors production traffic, on-demand evaluation lets you perform targeted assessments of selected interactions at any time.\n",
    "\n",
    "With on-demand evaluation, you specify the exact spans, traces or sessions you want to evaluate by providing their span, trace or session IDs. When using the AgentCore Starter toolkit you can also automatically evaluate all traces in a session. \n",
    "\n",
    "You can then apply custom evaluators or built-in evaluators to your agent's interactions. This evaluation type is particularly useful when you need to investigate specific customer interactions, validate fixes for reported issues, or analyze historical data for quality improvements. Once you submit the evaluation request, the service processes only the specified spans and provides detailed results for your analysis.\n",
    "\n",
    "### Generating traces on AgentCore Observability from an agent\n",
    "\n",
    "AgentCore Observability provides comprehensive visibility into agent behavior during invocations by leveraging [OpenTelemetry (OTEL)](https://opentelemetry.io/) traces as the foundation for capturing and structuring detailed execution data. AgentCore relies on [AWS Distro for OpenTelemetry (ADOT)](https://aws-otel.github.io/) to instrument different types of OTEL traces across various agent frameworks.\n",
    "\n",
    "When your agent is hosted on AgentCore Runtime (like our agent in this tutorial), the AgentCore Observability instrumentation is automatic, with minimal configuration. All you need to do is include `aws-opentelemetry-distro` in `requirements.txt` and AgentCore Runtime handles OTEL configuration automatically. When your agent is not running in AgentCore Runtime, you will need to instrument it with ADOT to have it available in AgentCore Observability. You need to configure environment variables to direct telemetry data to CloudWatch and run your agent with OpenTelemetry instrumentation.\n",
    "\n",
    "The process looks as following:\n",
    "\n",
    "![session_traces](../../images/observability_traces.png)\n",
    "\n",
    "Once your session traces are available in AgentCore Observability, you can use AgentCore Evaluations to evaluate your agent's behavior.\n",
    "\n",
    "### How on-demand evaluation works with the traces\n",
    "\n",
    "On the on-demand evaluation, your agent is invoked and generates traces in AgentCore Observability. Those traces are mapped to sessions and their logs are made available in Amazon CloudWatch Log groups. With the on-demand evaluation, a developer decides which sessions or traces to use and send those as inputs to AgentCore Evaluations, together with the metrics to evaluate the traces content. The process looks as following:\n",
    "\n",
    "\n",
    "![session_traces](../../images/on_demand_evaluations.png)\n",
    "\n",
    "### Retrieving information from previous tutorials\n",
    "\n",
    "For this tutorial, we will use the Strands agent deployed in AgentCore Runtime during our prerequisites tutorial. We will evaluate it with pre-built metrics and with the `response_quality` metric we created in the `01-creating-custom-metrics` tutorial. Let's retrieve our agent and evaluator informations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6877d3a-d584-4805-bbec-a8629ac288fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r launch_result_strands\n",
    "%store -r session_id_strands\n",
    "%store -r evaluator_id\n",
    "try:\n",
    "    print(\"Agent Id:\", launch_result_strands.agent_id)\n",
    "    print(\"Agent ARN:\", launch_result_strands.agent_arn)\n",
    "except NameError as e:\n",
    "    raise Exception(\"\"\"Missing launch results from your Strands agent. Please run 00-prereqs before executing this lab\"\"\")\n",
    "\n",
    "try:\n",
    "    print(\"Session id:\", session_id_strands)\n",
    "except NameError as e:\n",
    "    raise Exception(\"\"\"Missing session id from your Strands agent. Please run 00-prereqs before executing this lab\"\"\")\n",
    "\n",
    "try:\n",
    "    print(\"Evaluator id:\", evaluator_id)\n",
    "except NameError as e:\n",
    "    raise Exception(\"\"\"Missing custom evaluator id. Please run 01-creating-custom-evaluators before executing this lab\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c895803-39e1-443c-95b1-df9d0bf6d801",
   "metadata": {},
   "source": [
    "### Initiating the AgentCore Evaluations's client\n",
    "\n",
    "Now let's initiate the AgentCore Evaluations client from the AgentCore Starter toolkit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bedrock_agentcore_starter_toolkit import Evaluation, Observability\n",
    "import os\n",
    "import json\n",
    "from boto3.session import Session\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db7afa7-b1ca-43bf-a28b-6c1fcc8cdafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "boto_session = Session()\n",
    "region = boto_session.region_name\n",
    "print(region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2312c878-11ba-4cda-9afb-b5cfcabc4643",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_client = Evaluation(region=region)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a963ec2a-f2ad-45d4-9830-685dfbee024b",
   "metadata": {},
   "source": [
    "### Running evaluations\n",
    "\n",
    "To run AgentCore Evaluations, you must provide session, trace or span information. Different metrics require different level of information from your agent traces, as we saw in the previous tutorial.\n",
    "\n",
    "![metrics level](../../images/metrics_per_level.png)\n",
    "\n",
    "When you are using one of AWS's SDKs you will need to process your trace by yourself. The AgentCore Starter toolkit simplifies this process for you and processes your traces based on a session id or a trace id. This way, you can provide a session id for the AgentCore starter toolkit evaluator client during a `run` interaction and the sdk will extract and evaluate all the traces in that session for trace and span level metrics. Optionally, you can also provide the name of an output file to store your evaluation results. \n",
    "\n",
    "### Goal Success Rate\n",
    "\n",
    "Let's now evaluate the Goal Success Rate of our agent. Remember, we asked the agent the following questions:\n",
    "\n",
    "* What is the weather now?\n",
    "* How much is 2+2?\n",
    "* Can you tell me the capital of the US?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da9a1be-5e03-4fa3-8bf4-6a042b4ef23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "goal_sucess_results = eval_client.run(\n",
    "    agent_id=launch_result_strands.agent_id,\n",
    "    session_id=session_id_strands, \n",
    "    evaluators=[\"Builtin.GoalSuccessRate\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c6a01a-818f-45fb-9f23-456887685d12",
   "metadata": {},
   "source": [
    "Let's now understand the results of our evaluator. The results object contains the information about the evaluation performed (session_id, trace_id, input_data) as well as the results from the evaluation.\n",
    "\n",
    "The evaluation results include the evaluator information (id, name, ARN), the evaluation value, the evaluation label, the evaluation explanation and some extra context about the evaluation job (spanContext, token_usage, ...).\n",
    "\n",
    "Let's take a look at our evaluation response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9666eb-9c56-4bc3-acbb-d3c70ffee597",
   "metadata": {},
   "outputs": [],
   "source": [
    "for result in goal_sucess_results.results:\n",
    "    information = f\"\"\"\n",
    "    Goal Success: {result.label} ({result.value})\n",
    "    Explanation: \\n{result.explanation}]\\n\n",
    "    Token Usage: {result.token_usage}\\n\n",
    "    Context: {result.context}\\n\n",
    "    \"\"\"\n",
    "    display(Markdown(information))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c9cc0e-66c5-42fa-86e7-c568618aa7bd",
   "metadata": {},
   "source": [
    "### Correctness\n",
    "\n",
    "Let's now analyze the same session for a trace-level metric: Correctness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5000cd07-885d-4bc6-8d4f-aa9de3096d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "correctness_results = eval_client.run(\n",
    "    agent_id=launch_result_strands.agent_id,\n",
    "    session_id=session_id_strands, \n",
    "    evaluators=[\"Builtin.Correctness\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb2ef19-74e3-4d2a-8a96-5a08c934c153",
   "metadata": {},
   "source": [
    "Let's now understand the results of our evaluator. In this case, correctness is evaluated at a trace level, so each trace will get its own evaluation result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2752db8e-9089-4f4a-ad86-408758740ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for result in correctness_results.results:\n",
    "    information = f\"\"\"\n",
    "    Correctness: {result.label} ({result.value})\n",
    "    Explanation: \\n{result.explanation}]\\n\n",
    "    Token Usage: {result.token_usage}\\n\n",
    "    Context: {result.context}\\n\n",
    "    \"\"\"\n",
    "    display(Markdown(information))\n",
    "    print(\"================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76ba398-3c2a-44c2-9288-65acd3a67dd3",
   "metadata": {},
   "source": [
    "### Tool selection accuracy and parameter selection accuracy\n",
    "\n",
    "Let's now evaluate our agent for the tool and parameter selection. Both metrics are evaluated at the span level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e30a77-8309-4c62-87e4-deb96d1178a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_results = eval_client.run(\n",
    "    agent_id=launch_result_strands.agent_id,\n",
    "    session_id=session_id_strands, \n",
    "    evaluators=[\"Builtin.ToolParameterAccuracy\", \"Builtin.ToolSelectionAccuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088b53ea-6974-4731-b73a-36242e72d720",
   "metadata": {},
   "source": [
    "Let's now analyze the results. In this case, we are evaluating the session with two different metrics in the same run. That means that we now need to know which evaluator is producing each response. We can do that with the `evaluator_name` property of the result. Let's see how well our agent used tools:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b645bc-4897-4200-a25b-871786c1fac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for result in parameter_results.results:\n",
    "    information = f\"\"\"\n",
    "    Metric: {result.evaluator_name}\n",
    "    Value: {result.label} ({result.value})\n",
    "    Explanation: \\n{result.explanation}]\\n\n",
    "    Token Usage: {result.token_usage}\\n\n",
    "    Context: {result.context}\\n\n",
    "    \"\"\"\n",
    "    display(Markdown(information))\n",
    "    print(\"================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650fac68-1adb-4ce6-ab07-770405ac5532",
   "metadata": {},
   "source": [
    "### Using custom evaluator\n",
    "\n",
    "Now that we have used evaluators in the session, trace and span level, let's use our custom metric to evaluate our response quality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7282eb44-0611-4e9d-96ad-2b009f62e60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_results = eval_client.run(\n",
    "    agent_id=launch_result_strands.agent_id,\n",
    "    session_id=session_id_strands, \n",
    "    evaluators=[evaluator_id]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073e2b08-8007-49e2-af75-3aeef58956c3",
   "metadata": {},
   "source": [
    "Let's now take a look at the evaluation results. In this case, we are evaluating an agent that has the following instructions:\n",
    "\n",
    "```\n",
    "You're a helpful assistant. You can do simple math calculation, and tell the weather.\n",
    "```\n",
    "\n",
    "For our evaluation metric we are penalizing the agent for going out of scope with a `Very Poor` quality as stated in our evaluation instructions:\n",
    "\n",
    "```\n",
    "...\n",
    "**IMPORTANT**: A response quality can only be high if the agent remains in its original scope. Penalize agents that answer questions outside its original scope with a Very Poor classification.\n",
    "...\n",
    "```\n",
    "\n",
    "Since we are evaluating the following questions:\n",
    "\n",
    "* What is the weather now?\n",
    "* How much is 2+2?\n",
    "* Can you tell me the capital of the US?\n",
    "\n",
    "We expect the agent to have a `Very Poor` evaluation for the last question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42066be3-765e-4ec1-a728-e8da71197b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "for result in custom_results.results:\n",
    "    information = f\"\"\"\n",
    "    Metric: {result.evaluator_name}\n",
    "    Value: {result.label} ({result.value})\n",
    "    Explanation: \\n{result.explanation}]\\n\n",
    "    Token Usage: {result.token_usage}\\n\n",
    "    Context: {result.context}\\n\n",
    "    \"\"\"\n",
    "    display(Markdown(information))\n",
    "    print(\"================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c96505-abb6-4aa8-90cf-f047a5155435",
   "metadata": {},
   "source": [
    "### Saving evaluation results\n",
    "\n",
    "The AgentCore starter toolkit also helps you saving the results of your agent evaluation in structured output files. To do so all you need to provide is the `ouput` parameter during the run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17db14a-9bde-4ad9-95aa-c0ecc2db0194",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results = eval_client.run(\n",
    "    agent_id=launch_result_strands.agent_id,\n",
    "    session_id=session_id_strands, \n",
    "    evaluators=[\n",
    "        evaluator_id\n",
    "    ],\n",
    "    output=\"evals_results/output.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026bfcb6-0bb7-49e5-add2-e9ca3c677871",
   "metadata": {},
   "source": [
    "### Congrats!\n",
    "\n",
    "You have now evaluated your agent with the on-demand capabilities. In the next tutorial, we will automate the evaluation of the agent for a production environment by setting an online evaluator and connecting it with the agent."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

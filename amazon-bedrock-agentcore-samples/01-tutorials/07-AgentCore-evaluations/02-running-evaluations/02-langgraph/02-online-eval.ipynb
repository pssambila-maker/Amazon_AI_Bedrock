{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "## AgentCore Evaluations - online evaluation for LangGraph Agent\n",
    "\n",
    "In this tutorial you will learn about to use the online evaluation from AgentCore Evaluations applied to a LangGraph agent.\n",
    "\n",
    "To execute this lab you should first have created the LangGraph agent using the code at [00-prereqs](../../00-prereqs) folder and created your custom evaluator using the code at [01-creating-custom-evaluators](../../01-creating-custom-evaluators)\n",
    "\n",
    "### What You'll Learn\n",
    "- How to run online evaluations to a trace using the AgentCore Starter toolkit\n",
    "\n",
    "### Tutorial Details\n",
    "\n",
    "| Information         | Details                                                                       |\n",
    "|:--------------------|:------------------------------------------------------------------------------|\n",
    "| Tutorial type       | Evaluating LangGraph agent with online evaluators (built-in and custom)         |\n",
    "| Tutorial components | Setting automated evaluation with built-in and custom evaluators              |\n",
    "| Tutorial vertical   | Cross-vertical                                                                |\n",
    "| Example complexity  | Easy                                                                          |\n",
    "| SDK used            | Amazon Bedrock AgentCore Starter toolkit                                      |\n",
    "\n",
    "### Online evaluation\n",
    "\n",
    "Online evaluation enables live-traffic quality monitoring of deployed agents. Unlike on-demand evaluation which analyzes specific selected interactions, online evaluation continuously evaluates your agent's performance in production environments based on real-time traffic.\n",
    "\n",
    "Online evaluation consists of three main components. First, **session sampling and filtering** allows you to configure specific rules to evaluate agent interactions. You can set percentage-based sampling to evaluate a portion of all sessions (for example, 10%) or define conditional filters for more targeted evaluation. Second, you can choose from **multiple evaluation methods** including creating new custom evaluators, using existing custom evaluators, or selecting from built-in evaluators. Finally, the **monitoring and analysis** capabilities let you view aggregated scores in dashboards, track quality trends over time, investigate low-scoring sessions, and analyze complete interaction flows from input to output.\n",
    "\n",
    "With online evaluation, you configure the system to automatically monitor specific data sourcesâ€”either CloudWatch log groups containing agent traces or AgentCore Runtime endpoints. The service continuously processes incoming traces based on your sampling and filtering rules, applies your chosen evaluators in real-time, and outputs detailed results to CloudWatch for analysis. This evaluation type is particularly useful for production monitoring, catching quality regressions early, identifying patterns in user interactions, and maintaining consistent agent performance at scale.\n",
    "\n",
    "Once you create and enable an online evaluation configuration, the service runs continuously in the background, evaluating sessions as they occur and providing ongoing visibility into your agent's quality metrics. You can pause, modify, or delete configurations at any time to adapt your evaluation strategy as your needs evolve.\n",
    "\n",
    "### Generating traces on AgentCore Observability from an agent\n",
    "\n",
    "AgentCore Observability provides comprehensive visibility into agent behavior during invocations by leveraging [OpenTelemetry (OTEL)](https://opentelemetry.io/) traces as the foundation for capturing and structuring detailed execution data. AgentCore relies on [AWS Distro for OpenTelemetry (ADOT)](https://aws-otel.github.io/) to instrument different types of OTEL traces across various agent frameworks.\n",
    "\n",
    "When your agent is hosted on AgentCore Runtime (like our agent in this tutorial), the AgentCore Observability instrumentation is automatic, with minimal configuration. All you need to do is include `aws-opentelemetry-distro` in `requirements.txt` and AgentCore Runtime handles OTEL configuration automatically. When your agent is not running in AgentCore Runtime, you will need to instrument it with ADOT to have it available in AgentCore Observability. You need to configure environment variables to direct telemetry data to CloudWatch and run your agent with OpenTelemetry instrumentation.\n",
    "\n",
    "The process looks as following:\n",
    "\n",
    "![session_traces](../../images/observability_traces.png)\n",
    "\n",
    "Once your session traces are available in AgentCore Observability, you can use AgentCore Evaluations to evaluate your agent's behavior. For online evaluations, you don't need to do anything extra. Just monitor your agent's performance from the live dashboards.\n",
    "\n",
    "### How online evaluation works with the traces\n",
    "\n",
    "On the online evaluation, your agent is invoked and generates traces in AgentCore Observability. Those traces are mapped to sessions and their logs are made available in Amazon CloudWatch Log groups. With the online evaluation, a developer creates an online evaluation configuration for a certain agent and defines a sample rate and the evaluators to be applied for this configuration. AgentCore Evaluations will then automatically evaluate the agent in production, analyzing the produced traces according to the set sampling rate. The developer can then use the AgentCore Observability dashboards to visualize the traces and evaluation scores from the agent to continuously update the agent according to the evaluations results.\n",
    "\n",
    "\n",
    "![session_traces](../../images/online_evaluations.png)\n",
    "\n",
    "### Retrieving information from previous tutorials\n",
    "\n",
    "For this tutorial, we will use the LangGraph agent deployed in AgentCore Runtime during our prerequisites tutorial. We will evaluate it with pre-built metrics and with the `response_quality` metric we created in the `01-creating-custom-metrics` tutorial. Let's retrieve our agent and evaluator informations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30d7001-3212-44b6-b401-ea93b8a9abc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r launch_result_langgraph\n",
    "%store -r evaluator_id\n",
    "try:\n",
    "    print(\"Agent Id:\", launch_result_langgraph.agent_id)\n",
    "    print(\"Agent ARN:\", launch_result_langgraph.agent_arn)\n",
    "except NameError as e:\n",
    "    raise Exception(\"\"\"Missing launch results from your LangGraph agent. Please run 00-prereqs before executing this lab\"\"\")\n",
    "\n",
    "try:\n",
    "    print(\"Evaluator id:\", evaluator_id)\n",
    "except NameError as e:\n",
    "    raise Exception(\"\"\"Missing custom evaluator id. Please run 01-creating-custom-evaluators before executing this lab\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d01af13-5b54-45eb-93f7-31d4d0974cce",
   "metadata": {},
   "source": [
    "### Initiating the AgentCore Evaluations's client\n",
    "\n",
    "Now let's initiate the AgentCore Evaluations client from the AgentCore Starter toolkit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7ff2f6-d0ee-4d11-8776-d9dbbc729dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bedrock_agentcore_starter_toolkit import Evaluation, Observability\n",
    "import os\n",
    "import json\n",
    "from boto3.session import Session\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ebd476-2e81-476c-b328-36a60d7b199d",
   "metadata": {},
   "outputs": [],
   "source": [
    "boto_session = Session()\n",
    "region = boto_session.region_name\n",
    "print(region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06e1972-6cf3-4e07-adfd-cb4981902668",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_client = Evaluation(region=region)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddec431f-d5bd-47a1-b945-e29692dfaf14",
   "metadata": {},
   "source": [
    "### Setting online evaluation configuration\n",
    "\n",
    "Let's now set the online evaluation configuration. In this case, we will evaluate every trace produced as we are only using our agent for demonstration purposes. In real-life applications, you want to set the sample rate accordingly to your agent's utilization.\n",
    "\n",
    "We will create an evaluation configuration with the 5 metrics we explored in the on-demand evaluation:\n",
    "* Builtin.GoalSuccessRate\n",
    "* Builtin.Correctness\n",
    "* Builtin.ToolParameterAccuracy\n",
    "* Builtin.ToolSelectionAccuracy and\n",
    "* our custom metric: response_Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1921ef17-43e4-4471-92b9-a0f31bd8f018",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = eval_client.create_online_config(\n",
    "    agent_id=launch_result_langgraph.agent_id,\n",
    "    config_name=\"langgraph_agent_eval\",\n",
    "    sampling_rate=100,\n",
    "    evaluator_list=[\n",
    "        \"Builtin.GoalSuccessRate\", \"Builtin.Correctness\", \n",
    "        \"Builtin.ToolParameterAccuracy\", \"Builtin.ToolSelectionAccuracy\",\n",
    "        evaluator_id\n",
    "    ],\n",
    "    config_description=\"LangGraph agent online evaluation test\",\n",
    "    auto_create_execution_role=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07d6132-71d7-4654-9310-a61bdc435bc9",
   "metadata": {},
   "source": [
    "### Analyzing the evaluation configuration\n",
    "\n",
    "Let's see the configuration ID from our online evaluation configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eaf9173-400c-4ba2-9016-ebb956885819",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Online Evaluation Configuration Id:\", response['onlineEvaluationConfigId'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746ac193-5672-473d-a4c2-668667c1e77e",
   "metadata": {},
   "source": [
    "We can also see the details of the configuration created to confirm it is already enabled:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b8ee2f-f875-41a8-8226-650d1127e5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_client.get_online_config(config_id=response['onlineEvaluationConfigId'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257f892c-b15a-497e-a454-cb1677468725",
   "metadata": {},
   "source": [
    "### Invoking agent to trigger evaluation\n",
    "\n",
    "Let's now invoke our agent with a couple new queries to trigger our online evaluation. This time we will invoke our agent with boto3 as once the endpoint is available you can invoke it from any interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f88a905-767f-49e9-9978-66ae5845343f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "agentcore_client = boto3.client(\n",
    "    'bedrock-agentcore',\n",
    "    region_name=region\n",
    ")\n",
    "\n",
    "def invoke_agent_runtime(agent_arn, prompt):\n",
    "    boto3_response = agentcore_client.invoke_agent_runtime(\n",
    "        agentRuntimeArn=agent_arn,\n",
    "        qualifier=\"DEFAULT\",\n",
    "        payload=json.dumps({\"prompt\": prompt})\n",
    "    )\n",
    "    if \"text/event-stream\" in boto3_response.get(\"contentType\", \"\"):\n",
    "        content = []\n",
    "        for line in boto3_response[\"response\"].iter_lines(chunk_size=1):\n",
    "            if line:\n",
    "                line = line.decode(\"utf-8\")\n",
    "                if line.startswith(\"data: \"):\n",
    "                    line = line[6:]\n",
    "                    print(line)\n",
    "                    content.append(line)\n",
    "        display(Markdown(\"\\n\".join(content)))\n",
    "    else:\n",
    "        try:\n",
    "            events = []\n",
    "            for event in boto3_response.get(\"response\", []):\n",
    "                events.append(event)\n",
    "        except Exception as e:\n",
    "            events = [f\"Error reading EventStream: {e}\"]\n",
    "        display(Markdown(json.loads(events[0].decode(\"utf-8\"))))\n",
    "    return boto3_response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531b72f0-8c4b-4b27-b0ca-38c22d666cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = invoke_agent_runtime(\n",
    "    launch_result_langgraph.agent_arn,\n",
    "    \"How much is 7+9+10*2?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbda8d0-dc8d-4793-90ed-63760b0ab094",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = invoke_agent_runtime(\n",
    "    launch_result_langgraph.agent_arn,\n",
    "    \"Is it raining?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de72c8a-0ac8-40af-9604-b5a730ef803d",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = invoke_agent_runtime(\n",
    "    launch_result_langgraph.agent_arn,\n",
    "    \"how much is 20% of 300?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f903cc1-e025-4a67-bfeb-eed6b8f6f326",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = invoke_agent_runtime(\n",
    "    launch_result_langgraph.agent_arn,\n",
    "    \"What can you do?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d85101-dd53-40a0-8d42-403742c7eed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = invoke_agent_runtime(\n",
    "    launch_result_langgraph.agent_arn,\n",
    "    \"What is the capital of NY State?\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f7fc5937-b248-47bc-a596-48f46728bb13",
   "metadata": {},
   "source": [
    "### Visualizing Online Evaluation\n",
    "\n",
    "Once you create enought interactions with your agent you can use the [AgentCore Observability console ](https://console.aws.amazon.com/cloudwatch/home#gen-ai-observability/agent-core/agents) to visualize how it is performing according to your online evaluation configuration. \n",
    "\n",
    "Navigate to your agent `DEFAULT` endpoint to see the current evaluations\n",
    "\n",
    "**Important**: The evaluation results might take a while to appear in your dashboard. If you evaluation dashboard is empty, please wait a couple of minutes to check it again.\n",
    "\n",
    "Once available you will be able to see your metrics directly in the agent's traces:\n",
    "![image.png](../../images/online_evaluations_dashboard.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4bbc31-8f9c-4685-9db4-e59a500c78f3",
   "metadata": {},
   "source": [
    "### Congratulations!\n",
    "\n",
    "You have created your first Online Evaluation Configuration! You can now create custom metrics and evaluate your agent on-demand and online with AgentCore Evaluations!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c0122e65c053f38",
   "metadata": {},
   "source": [
    "# Strands Agent with OpenLIT Observability on Amazon Bedrock AgentCore Runtime\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook demonstrates deploying a Strands agent to Amazon Bedrock AgentCore Runtime with OpenLIT observability integration. The implementation uses Amazon Bedrock Claude models and sends telemetry data to [OpenLIT](https://github.com/openlit/openlit) through OpenTelemetry (OTEL).\n",
    "\n",
    "## Key Components\n",
    "\n",
    "- **Strands Agents**: Python framework for building LLM-powered agents with built-in telemetry support\n",
    "- **Amazon Bedrock AgentCore Runtime**: Managed runtime service for hosting and scaling agents on AWS\n",
    "- **OpenLIT**: Open-source observability platform for LLM applications and AI Agents built on OpenTelemetry\n",
    "- **OpenTelemetry**: Industry-standard protocol for collecting and exporting telemetry data\n",
    "\n",
    "## Architecture\n",
    "\n",
    "The agent is containerized and deployed to AgentCore Runtime, which provides HTTP endpoints for invocation. Telemetry data flows from the Strands agent through OTEL exporters to OpenLIT for monitoring and debugging. The implementation disables AgentCore's default observability to use OpenLIT instead.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Python 3.10+\n",
    "- AWS credentials configured with Bedrock and AgentCore permissions\n",
    "- [OpenLIT](https://github.com/openlit/openlit) deployed\n",
    "- Docker installed locally\n",
    "- Access to Amazon Bedrock Claude models in us-west-2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def21rvknub",
   "metadata": {},
   "source": [
    "## OpenLIT Setup\n",
    "\n",
    "Before proceeding with the agent deployment, you need to set up OpenLIT to receive telemetry data. OpenLIT must be accessible from Amazon Bedrock AgentCore Runtime.\n",
    "\n",
    "### Deployment Options\n",
    "\n",
    "You have two main options for deploying OpenLIT:\n",
    "\n",
    "#### Option 1: Docker Deployment (Quickest for Testing)\n",
    "Deploy OpenLIT using Docker Compose. This is the simplest approach for getting started:\n",
    "\n",
    "```bash\n",
    "# Using Docker Compose (recommended for quick setup)\n",
    "git clone https://github.com/openlit/openlit.git\n",
    "cd openlit\n",
    "docker compose up -d\n",
    "```\n",
    "\n",
    "This will start OpenLIT on `http://localhost:3000` (UI) and `http://localhost:4318` (OTEL endpoint). \n",
    "\n",
    "For AgentCore to access it, you need to deploy this on a machine (e.g., EC2 instance) that AgentCore can reach:\n",
    "1. Deploy on an EC2 instance or container service with a public IP\n",
    "2. Ensure port 4318 is accessible (configure security groups to allow inbound traffic)\n",
    "3. Use the public endpoint URL (e.g., `http://<ec2-public-ip>:4318`) in your AgentCore configuration\n",
    "\n",
    "#### Option 2: Kubernetes Deployment (Production-Ready)\n",
    "For production use, deploy OpenLIT on Kubernetes using Helm:\n",
    "\n",
    "```bash\n",
    "# Add OpenLIT Helm repository\n",
    "helm repo add openlit https://openlit.github.io/helm-charts\n",
    "helm repo update\n",
    "\n",
    "# Install OpenLIT\n",
    "helm install openlit openlit/openlit\n",
    "```\n",
    "\n",
    "**Default Configuration:**\n",
    "- By default, OpenLIT creates a LoadBalancer service with a public IP\n",
    "- This makes the OTEL endpoint publicly accessible at `http://<load-balancer-ip>:4318`\n",
    "- The UI will be accessible at `http://<load-balancer-ip>:3000`\n",
    "\n",
    "**VPC/Private Configuration:**\n",
    "If you prefer to keep OpenLIT private within a VPC (recommended for production):\n",
    "\n",
    "1. **Deploy in the same VPC as AgentCore** or configure VPC peering\n",
    "2. **Change service type to ClusterIP or use internal Load Balancer:**\n",
    "   ```bash\n",
    "   helm install openlit openlit/openlit \\\n",
    "     --set service.type=ClusterIP\n",
    "   ```\n",
    "   Or for AWS internal load balancer:\n",
    "   ```bash\n",
    "   helm install openlit openlit/openlit \\\n",
    "     --set service.annotations.\"service\\.beta\\.kubernetes\\.io/aws-load-balancer-internal\"=\"true\"\n",
    "   ```\n",
    "3. **Configure security groups/network policies** to allow traffic between AgentCore and OpenLIT\n",
    "4. Use the internal endpoint (e.g., `http://openlit.default.svc.cluster.local:4318` or internal load balancer DNS)\n",
    "\n",
    "### Getting Your OpenLIT Endpoint\n",
    "\n",
    "Once OpenLIT is deployed, you'll use the OTEL endpoint in the format:\n",
    "- **Docker with public IP**: `http://<ec2-public-ip-or-domain>:4318`\n",
    "- **Kubernetes with public LoadBalancer**: `http://<load-balancer-external-ip>:4318`\n",
    "- **Kubernetes with internal/VPC**: `http://<internal-dns-or-ip>:4318`\n",
    "\n",
    "üìù **Save this endpoint URL** - you'll need it in the configuration step below.\n",
    "\n",
    "For detailed deployment instructions, refer to the [OpenLIT Installation Guide](https://docs.openlit.io/latest/openlit/installation)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8gaz7cxbgox",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "Install required dependencies from the requirements.txt file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --force-reinstall -U -r requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932110e6-fca6-47b6-b7c5-c4714a866a80",
   "metadata": {},
   "source": [
    "## Agent Implementation\n",
    "\n",
    "The agent file (`strands_claude.py`) implements a travel agent with web search capabilities. Key configuration includes:\n",
    "- Initializing Strands telemetry with OTLP exporter\n",
    "- Using lazy initialization to ensure environment variables are loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b845b32-a03e-45c2-a2f0-2afba8069f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile strands_claude.py\n",
    "import os\n",
    "import logging\n",
    "from bedrock_agentcore.runtime import BedrockAgentCoreApp\n",
    "from strands import Agent, tool\n",
    "from strands.models import BedrockModel\n",
    "from strands.telemetry import StrandsTelemetry\n",
    "from ddgs import DDGS\n",
    "\n",
    "logging.basicConfig(level=logging.ERROR, format=\"[%(levelname)s] %(message)s\")\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(os.getenv(\"AGENT_RUNTIME_LOG_LEVEL\", \"INFO\").upper())\n",
    "\n",
    "\n",
    "@tool\n",
    "def web_search(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Search the web for information using DuckDuckGo.\n",
    "\n",
    "    Args:\n",
    "        query: The search query\n",
    "\n",
    "    Returns:\n",
    "        A string containing the search results\n",
    "    \"\"\"\n",
    "    try:\n",
    "        ddgs = DDGS()\n",
    "        results = ddgs.text(query, max_results=5)\n",
    "\n",
    "        formatted_results = []\n",
    "        for i, result in enumerate(results, 1):\n",
    "            formatted_results.append(\n",
    "                f\"{i}. {result.get('title', 'No title')}\\n\"\n",
    "                f\"   {result.get('body', 'No summary')}\\n\"\n",
    "                f\"   Source: {result.get('href', 'No URL')}\\n\"\n",
    "            )\n",
    "\n",
    "        return \"\\n\".join(formatted_results) if formatted_results else \"No results found.\"\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"Error searching the web: {str(e)}\"\n",
    "\n",
    "# Function to initialize Bedrock model\n",
    "def get_bedrock_model():\n",
    "    region = os.getenv(\"AWS_DEFAULT_REGION\", \"us-west-2\")\n",
    "    model_id = os.getenv(\"BEDROCK_MODEL_ID\", \"us.anthropic.claude-3-7-sonnet-20250219-v1:0\")\n",
    "\n",
    "    bedrock_model = BedrockModel(\n",
    "        model_id=model_id,\n",
    "        region_name=region,\n",
    "        temperature=0.0,\n",
    "        max_tokens=1024\n",
    "    )\n",
    "    return bedrock_model\n",
    "\n",
    "# Initialize the Bedrock model\n",
    "bedrock_model = get_bedrock_model()\n",
    "\n",
    "# Define the agent's system prompt\n",
    "system_prompt = \"\"\"You are an experienced travel agent specializing in personalized travel recommendations \n",
    "with access to real-time web information. Your role is to find dream destinations matching user preferences \n",
    "using web search for current information. You should provide comprehensive recommendations with current \n",
    "information, brief descriptions, and practical travel details.\"\"\"\n",
    "\n",
    "app = BedrockAgentCoreApp()\n",
    "\n",
    "def initialize_agent():\n",
    "    \"\"\"Initialize the agent with proper telemetry configuration.\"\"\"\n",
    "\n",
    "    # Initialize Strands telemetry with 3P configuration\n",
    "    strands_telemetry = StrandsTelemetry()\n",
    "    strands_telemetry.setup_otlp_exporter()\n",
    "    \n",
    "    # Create and cache the agent\n",
    "    agent = Agent(\n",
    "        model=bedrock_model,\n",
    "        system_prompt=system_prompt,\n",
    "        tools=[web_search]\n",
    "    )\n",
    "    \n",
    "    return agent\n",
    "\n",
    "@app.entrypoint\n",
    "def strands_agent_bedrock(payload, context=None):\n",
    "    \"\"\"\n",
    "    Invoke the agent with a payload\n",
    "    \"\"\"\n",
    "    user_input = payload.get(\"prompt\")\n",
    "    logger.info(\"[%s] User input: %s\", context.session_id, user_input)\n",
    "    \n",
    "    # Initialize agent with proper configuration\n",
    "    agent = initialize_agent()\n",
    "    \n",
    "    response = agent(user_input)\n",
    "    return response.message['content'][0]['text']\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680f3c05",
   "metadata": {},
   "source": [
    "### Configure AgentCore Runtime deployment\n",
    "\n",
    "Next we will use our starter toolkit to configure the AgentCore Runtime deployment with an entrypoint, the execution role we just created and a requirements file. We will also configure the starter kit to auto create the Amazon ECR repository on launch.\n",
    "\n",
    "During the configure step, your docker file will be generated based on your application code. Please note that when using the `bedrock_agentcore_starter_toolkit` to configure your agent, it configures AgentCore Observability by default so, to use OpenLIT, you need to remove configuration for AgentCore Observability as explained below:\n",
    "\n",
    "<div style=\"text-align:left\">\n",
    "    <img src=\"../images/configure.png\" width=\"40%\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e79eba2-ca59-463f-9ebf-56e362d7ae66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bedrock_agentcore_starter_toolkit import Runtime\n",
    "from boto3.session import Session\n",
    "boto_session = Session()\n",
    "region = boto_session.region_name\n",
    "\n",
    "agentcore_runtime = Runtime()\n",
    "agent_name = \"strands_openlit_observability\"\n",
    "response = agentcore_runtime.configure(\n",
    "    entrypoint=\"strands_claude.py\",\n",
    "    auto_create_execution_role=True,\n",
    "    auto_create_ecr=True,\n",
    "    requirements_file=\"requirements.txt\",\n",
    "    region=region,\n",
    "    agent_name=agent_name,\n",
    "    disable_otel=True,\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4vnazx93s",
   "metadata": {},
   "source": [
    "## Deploy to AgentCore Runtime\n",
    "\n",
    "Now that we've got a docker file, let's launch the agent to the AgentCore Runtime. This will create the Amazon ECR repository and the AgentCore Runtime\n",
    "\n",
    "<div style=\"text-align:left\">\n",
    "    <img src=\"../images/launch.png\" width=\"75%\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a32ab8-7701-4900-8055-e24364bdf35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenLIT configuration\n",
    "# IMPORTANT: Replace with your actual OpenLIT OTEL endpoint\n",
    "# Examples:\n",
    "#   - Public EC2: \"http://ec2-XX-XXX-XXX-XXX.compute-1.amazonaws.com:4318\"\n",
    "#   - VPC/Private: \"http://10.0.1.100:4318\" or \"http://openlit.internal:4318\"\n",
    "\n",
    "otel_endpoint = \"http://<your-openlit-host>:4318\"  # ‚ö†Ô∏è REPLACE THIS with your OpenLIT endpoint\n",
    "\n",
    "env_vars = {\n",
    "    \"BEDROCK_MODEL_ID\": \"us.anthropic.claude-3-7-sonnet-20250219-v1:0\",  # Example model ID\n",
    "    \"OTEL_EXPORTER_OTLP_ENDPOINT\": otel_endpoint,  # Use OpenLIT OTEL endpoint\n",
    "    \"DISABLE_ADOT_OBSERVABILITY\": \"true\",\n",
    "}\n",
    "\n",
    "launch_result = agentcore_runtime.launch(env_vars=env_vars)\n",
    "launch_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ae9c09-09db-4a76-871a-92eacd96b9c3",
   "metadata": {},
   "source": [
    "## Check Deployment Status\n",
    "\n",
    "Wait for the runtime to be ready before invoking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa6ac09-9adb-4846-9fc1-4d12aeb74853",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "status_response = agentcore_runtime.status()\n",
    "status = status_response.endpoint['status']\n",
    "end_status = ['READY', 'CREATE_FAILED', 'DELETE_FAILED', 'UPDATE_FAILED']\n",
    "while status not in end_status:\n",
    "    time.sleep(10)\n",
    "    status_response = agentcore_runtime.status()\n",
    "    status = status_response.endpoint['status']\n",
    "    print(status)\n",
    "status"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f89c56-918a-4cab-beaa-c7ac43a2ba29",
   "metadata": {},
   "source": [
    "### Invoking AgentCore Runtime\n",
    "\n",
    "Finally, we can invoke our AgentCore Runtime with a payload\n",
    "\n",
    "<div style=\"text-align:left\">\n",
    "    <img src=\"../images/invoke.png\" width=75%\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d909e42-e1a0-407f-84c2-3d16cc889cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "invoke_response = agentcore_runtime.invoke({\"prompt\": \"I'm planning a weekend trip to Tokyo. What are the must-visit places and local food I should try?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc34c4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "display(Markdown(\"\".join(invoke_response['response'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05efe60e",
   "metadata": {},
   "source": [
    "## View Traces in OpenLIT\n",
    "\n",
    "To view the traces:\n",
    "1. Access your OpenLIT dashboard:\n",
    "   - For self-hosted: Navigate to `http://your-openlit-host:3000`\n",
    "2. Click on \"Requests\" section\n",
    "3. Filter by your service name\n",
    "\n",
    "The traces will include:\n",
    "- Agent invocation details with full request/response context\n",
    "- Tool calls (web search) with execution time\n",
    "- Model interactions with latency, token usage, and cost estimates\n",
    "- Request/response payloads\n",
    "- Error tracking and debugging information\n",
    "- Performance metrics and analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3fdfe404469632",
   "metadata": {},
   "source": [
    "## Cleanup (Optional)\n",
    "\n",
    "Clean up the deployed resources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a6cf1416830a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "agentcore_control_client = boto3.client(\n",
    "    'bedrock-agentcore-control',\n",
    "    region_name=region\n",
    ")\n",
    "\n",
    "ecr_client = boto3.client(\n",
    "    'ecr',\n",
    "    region_name=region\n",
    ")\n",
    "\n",
    "runtime_delete_response = agentcore_control_client.delete_agent_runtime(\n",
    "    agentRuntimeId=launch_result.agent_id,\n",
    ")\n",
    "\n",
    "response = ecr_client.delete_repository(\n",
    "    repositoryName=launch_result.ecr_uri.split('/')[1],\n",
    "    force=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b118ad38-feeb-4d1d-9d57-e5c845becc56",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You have successfully deployed a Strands agent to Amazon Bedrock AgentCore Runtime with OpenLIT observability. The implementation demonstrates:\n",
    "- Integration of Strands agents with AgentCore Runtime\n",
    "- Configuration of OpenTelemetry to send traces to OpenLIT\n",
    "- Proper initialization order to ensure telemetry configuration\n",
    "- Invocation through both SDK and boto3 client\n",
    "\n",
    "The agent is now running in a managed, scalable environment with full observability through OpenLIT. OpenLIT provides comprehensive monitoring including:\n",
    "- Real-time trace visualization\n",
    "- Cost tracking and analysis\n",
    "- Performance metrics\n",
    "- Error tracking and debugging\n",
    "- Token usage analytics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
